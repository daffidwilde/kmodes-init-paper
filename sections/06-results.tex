To give comparative results on the quality of the initialisation processes 
defined in
Sections~\ref{sec:init},~\ref{sec:proposed-method}~\&~\ref{sec:preferences},
four well-known, categorical, labelled datasets \- soybean, mushroom, breast
cancer, and zoo animal \- will be clustered by the \(k\)-modes algorithm with
each of the initialisation processes. These datasets have been chosen to fall in
line with the established literature and for their relative sizes and
complexities.\\

Typically, the quality of a clustering algorithm is measured by its performance
at classifying datasets [\textbf{CITE}]. We will follow this approach to be
consistent with the literature, but we will also consider the quality of each
initialisation in terms of the clustering it produces on the whole dataset. The
hope of this entire work is that by deliberately choosing a more considered and
arguably fairer starting point, we should see a positive impact on the quality
of the clustering the algorithm produces.\\

For the purposes of measuring the performance of \(k\)-modes as a classifier, we
will split each dataset into two parts: approximately two thirds for training
the algorithm, leaving the remaining third unclustered. This third will then be
assigned to its nearest cluster, giving a predicted labelling. For computational
reasons, we will not make use of the typical classification metrics when
discussing prediction quality, i.e.\ accuracy, recall and precision. Instead, we
will opt for their less label-specific counterparts: the adjusted Rand index and
mutual information scores; homogeneity; and completeness. The most important
criterion of these metrics being used is taken to be that the true labelling of
our datasets is utilised and the metrics can be considered as external measures
on the validity of our algorithm's clustering.\\

On the other side of the coin, when gauging the performance of each
initialisation process as part of a clustering algorithm, we want internal
metrics that are independent of any external information such as a labelling.
Such metrics are typically built up from two characteristics of the clusters
found: cohesion and separation. Cluster cohesion is effectively the summed,
within-cluster variation or dissimilarity of its points, whereas a cluster's
separation is a sum of the distances between all points in the cluster and every
other point not in the cluster. In this analysis, we will make use of two
internal measures for cluster validity: our cost function from
Definition~\ref{def:cost} and the average silhouette coefficient of our
clustering.

\subsection{Performance metrics}\label{subsec:metrics}

\begin{definition}\label{def:contingency}
    Let \textbf{X} be a given dataset and consider two clusterings of this
    dataset:
    \[
        U = \left\{U_1, \ldots, U_r\right\}
        \ \text{and} \
        V = \left\{V_1, \ldots, V_s\right\}
    \]

    We do not require that \(r = s\). Now we define a \emph{contingency table},
    denoted by \(\left[n_{ij}\right]\), to summarise the similarity between our
    two clusterings. This table has entries given by
    \(n_{ij}~=~|U_i~\cap~V_j|\), and a representation of this table is shown in
    Table~\ref{tab:contingency}.

    \begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        {} & \(V_1\) & \(V_2\) & \(\cdots\) & \(V_s\) & Total
        \\ \midrule
        \(U_1\) & \(n_{11}\) & \(n_{12}\) & \(\cdots\) & \(n_{1s}\) & \(a_1\)
        \\
        \(U_2\) & \(n_{21}\) & \(n_{22}\) & \(\cdots\) & \(n_{2s}\) & \(a_2\)
        \\
        \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\ddots\) & \(\vdots\) &
        \(\vdots\)
        \\
        \(U_r\) & \(n_{r1}\) & \(n_{r2}\) & \(\cdots\) & \(n_{rs}\) & \(a_r\)
        \\ \midrule
        Total & \(b_1\) & \(b_2\) & \(\cdots\) & \(b_s\) & {}
    \end{tabular}
    \caption{The contingency table for two clusterings \(U, V\) of a dataset
    \textbf{X}.}\label{tab:contingency}
    \end{table}

    In addition to these entries, we have taken the sums of the each row and of
    each column, and denoted them by \(a\)~and~\(b\), respectively. That is:
    \[
        a_i = \sum_{j=1}^s n_{ij} \ \forall i = 1, \ldots, r, \quad \text{and}
        \quad b_j = \sum_{i=1}^r n_{ij} \ \forall j = 1, \ldots, s
    \]\\
\end{definition}

\begin{definition}\label{def:adjusted-rand-index}
    Let \textbf{X} be a dataset, \(U, V\) be two clusterings of the elements of
    \textbf{X}, and \(\left[n_{ij}\right]\) be the corresponding contingency
    table. Then the adjusted Rand index of these clusterings is given by:
    \[
        ARI(U, V) = \frac{\displaystyle{\sum_{ij} {n_{ij}\choose 2} -
        \frac{1}{{N\choose 2}}\left[\sum_i {a_i\choose 2}\sum_j {b_j\choose
        2}\right]}}{\displaystyle{\frac{1}{2} \left[\sum_i {a_i\choose 2} +
        \sum_j{b_j\choose 2}\right] - \frac{1}{{N\choose 2}}\left[\sum_i
        {a_i\choose 2}\sum_j {b_j\choose 2}\right]}}
    \]
\end{definition}

\begin{remark}
    In the case where one of \(U\) or \(V\) is a class label of \textbf{X}, we
    can interpret the adjusted Rand index as being a higher-level classification
    accuracy measure; `higher-level' in the sense that it is invariant of how
    our clustering algorithm labels its clusters. The adjusted Rand index
    considers the relationship between pairs of points in both clusterings of
    the dataset rather than each point individually.\\
    
    Say, without loss of generality, that \(V\) is a class label of \textbf{X}
    and \(U\) is some clustering of \textbf{X}. Then it follows that the
    adjusted Rand index takes value \(1\) if \(U\) and \(V\) are indentical (up
    to a permutation of the class labels), \(0\) if \(U\) is as good as a random
    clustering of \textbf{X}, and can take values less than \(0\) if the
    clustering \(U\) is worse than random.\\
\end{remark}

\begin{definition}\label{def:adjusted-mutual-info}
    Let \textbf{X} be a dataset, \(U, V\) be two clusterings of the elements of
    \textbf{X}, and \(\left[n_{ij}\right]\) be the corresponding contingency
    table. Then we define the following quantities:
    \begin{itemize}
        \item Consider a clustering \(C = \left\{C_1, \ldots, C_k\right\}\) of
            \textbf{X}. Then the \emph{entropy}, denoted by \(H(C)\), which is
            associated with this clustering is defined to be:
            \[
                H(C) = - \sum_{i=1}^k P(C_i)\log_2 P(C_i), \quad \text{where}
                \quad P(C_i) = \frac{\left|C_i\right|}{N}
            \]

            \(H(C)\) is non-negative and only takes the value \(0\) when
            \(k~=~1\), i.e.\ there is only one cluster. We can also interpret a
            class labelling of our dataset as a clustering, and so this too will
            have an entropy associated with it.

        \item Consider our two clusterings \(U, V\). We wish to quantify the
            information shared by both clusterings. This quantity is called the
            \emph{mutual information} between these clusterings, and is defined
            to be:
            \[
                MI(U, V) = \sum_{i=1}^r \sum_{j=1}^s P(U_i, V_j)\log_2
                \frac{P(U_i, V_j)}{P(U_i)P(V_j)}, \quad \text{where} \quad P(U_i,
                V_j) = \frac{\left|U_i \cap V_j\right|}{N}
            \]

            \(MI(U, V)\) is also non-negative and is bounded from above by
            \(H(U)\) and \(H(V)\).

        \item Using a combinatorial approach (in a similar fashion to
            Definition~\ref{def:adjusted-rand-index}), and a hypergeometric
            model of randomness, we define the
            \emph{expected~mutual~information} of two random clusterings to be:
            \[
                \mathbb{E}\left(MI(U, V)\right) = \sum_{i=1}^r \sum_{j=1}^s
                \sum_{n_{ij}=n_{ij}^*}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log_2
                \left(\frac{n_{ij} N}{a_i b_j}\right) \times \frac{a_i!\, b_j!\,
                \left(N - a_i\right)!\, \left(N - b_j\right)!}{N!\, n_{ij}!\,
                \left(a_i - n_{ij}\right)!\, \left(b_j - n_{ij}\right)!\,
                \left(N - a_i - b_j + n_{ij}\right)!}
            \]
            Here, we have \(n_{ij}^* = \max(1, a_i + b_j - N)\).\\
    \end{itemize}
    
    With these quantities, we define the \emph{adjusted mutual information} of
    our pair of clusterings to be:
    \[
        AMI(U, V) = \frac{MI(U, V) - \mathbb{E}\left(MI(U, V)\right)}{\max
        \left(H(U), H(V)\right) - \mathbb{E}\left(MI(U, V)\right)}
    \]
\end{definition}

\begin{remark}
    This adjusted quantity takes values in the interval \(\left[0, 1\right]\).
    In fact, \(AMI(U, V)~=~1\) when \(U\) and \(V\) are indentical, and
    \(AMI(U, V)~=~0\) when the mutual information between the two clusterings is
    equal to the expectation of the mutual information (i.e.\ two random
    cluserings of the data), by chance alone.\\
\end{remark}

\begin{definition}\label{def:homogeneity}
    Let \textbf{X} be a dataset with class labelling \(V\), let \(U\) be a
    clustering of \textbf{X} and let \(\left[n_{ij}\right]\) be the
    corresponding contingency table. The \emph{homogeneity} of \(U\) is defined
    to be:
    \[
        Hom(U) = \begin{cases}
                    \frac{MI(U, V)}{H(V)}, & \ \text{if} \ U \neq V \\
                    1, & \ \text{otherwise}
                 \end{cases}
    \]
    
    Here, \(MI(U, V)\) is the mutual information shared between \(U\) and \(V\),
    and \(H(V)\) is the entropy of the class labelling \(V\).\\

    We interpret the homogeneity of a clustering to be a measure of how `pure'
    each of its cluster are. That is, a clustering satisfies homogeneity if each
    of its clusters only contain points of a single class.\\
\end{definition}

\begin{definition}\label{def:completeness}
    Let \textbf{X} be a dataset with class labelling \(V\), let \(U\) be a
    clustering of \textbf{X} and let \(\left[n_{ij}\right]\) be the
    corresponding contingency table. The \emph{completeness} of \(U\) is defined
    to be:
    \[
        Com(U) = \begin{cases}
                    \frac{MI(U, V)}{H(U)}, & \ \text{if} \ U \neq V \\
                    1, & \ \text{otherwise}
                 \end{cases}
    \]
    
    Here, \(MI(U, V)\) is the mutual information shared between \(U\) and \(V\),
    and \(H(U)\) is the entropy of our clustering, \(U\).\\

    We say that a clustering is `complete' if all the elements of each class
    from \textbf{X} are in the same clusters in \(U\).\\
\end{definition}

\begin{definition}\label{def:silhouette}
    Let \textbf{X} be a dataset and consider a clustering of \textbf{X} into
    \(k\) parts, denoted by \(C = \left\{C_1, \ldots, C_k\right\}\). For each
    \(X^{(i)} \in \textbf{X}\), we define the following two quantities:
    \begin{itemize}
        \item Let \(a\left(X^{(i)}\right)\) denote the average dissimilarity
            between \(X^({i})\) and every other point in its cluster. Without
            loss of generality, let \(X^{(i)} \in C_l\). Then:
            \[
                a\left(X^{(i)}\right) := \frac{1}{|C_l|} D\left(C_l,
                X^{(i)}\right)
            \]
        \item Let \(b\left(X^{(i)}\right)\) denote the lowest average 
            dissimilarity between \(X^{(i)}\) and all other points in each
            cluster other than \(C_l\). That is:
            \[
                b\left(X^{(i)}\right) := \min_{l' \neq l} \left\{
                \frac{1}{|C_{l'}|} D\left(C_{l'}, X^{(i)}\right) \right\}
            \]\\
    \end{itemize}

    With these quantities we define, for each point in our datset, their
    \emph{silhouette coefficient}, denoted by \(s(X^{(i)})\):
    \[
        s(X^{(i)}) := \frac{a\left(X^{(i)}\right) -
        b\left(X^{(i)}\right)}{\max\left\{a\left(X^{(i)}\right),
        b\left(X^{(i)}\right)\right\}}
    \]

    The \emph{silhouette score} of a clustering \(C\) is simply the average of
    all the silhouette coefficients.
\end{definition}


\subsection{The datasets}\label{subsec:datasets}

A bit on the structure of each dataset and links to access them.

\subsection{Results}\label{subsec:results}

Tables of results for each dataset. Credit to
\url{https://github.com/nicodv/kmodes} for the Python implementation of both the
Huang and Cao processes, as well as the $k$-modes algorithm itself.
