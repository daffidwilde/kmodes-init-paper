To give comparative results on the quality of the initialisation processes 
defined in
Sections~\ref{sec:init},~\ref{sec:proposed-method}~\&~\ref{sec:preferences},
four well-known, categorical, labelled datasets \- soybean, mushroom, breast
cancer, and zoo animal \- will be clustered with the \(k\)-modes algorithm with
each of the initialisation processes. Rather than comparing our algorithms with
the typical metrics for classification algorithms (accuracy, precision and
recall) the comparison will be based on four clustering performance measures:
adjusted Rand index, adjusted mutual information, homegeneity and completeness.
Definitions of these metrics are given below. In addition to these, we will also
consider the final cost and number of iterations of the best runs for each
initialisation process. As a general rule, each algorithm will be trained on
approximately two thirds of the respective dataset and tested against the final
third.

\subsection{Performance metrics}\label{subsec:metrics}

Usually in the comparison of \(k\)-modes initialisation
processes~\cite{Huang98}\cite{Cao09}, metrics for gauging the quality of
classification algorithms are typically used. As \(k\)-modes is not a
classification algorithm but one for clustering datasets, we will utilise less
class-specific metrics. In their place are metrics built around more general
characteristics of a given clustering. In some respects, however, the metrics
defined below have been considered similar to their classification
counterparts.\\

\begin{definition}\label{def:contingency}
    Let \textbf{X} be a given dataset and consider two clusterings of this
    dataset:
    \[
        U = \left\{U_1, \ldots, U_r\right\}
        \ \text{and} \
        V = \left\{V_1, \ldots, V_s\right\}
    \]

    We do not require that \(r = s\). Now we define a \emph{contingency table},
    denoted by \(\left[n_{ij}\right]\), to summarise the similarity between our
    two clusterings. This table has entries given by
    \(n_{ij}~=~|U_i~\cap~V_j|\), and a representation of this table is shown in
    Table~\ref{tab:contingency}.

    \begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        {} & \(V_1\) & \(V_2\) & \(\cdots\) & \(V_s\) & Total
        \\ \midrule
        \(U_1\) & \(n_{11}\) & \(n_{12}\) & \(\cdots\) & \(n_{1s}\) & \(a_1\)
        \\
        \(U_2\) & \(n_{21}\) & \(n_{22}\) & \(\cdots\) & \(n_{2s}\) & \(a_2\)
        \\
        \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\ddots\) & \(\vdots\) &
        \(\vdots\)
        \\
        \(U_r\) & \(n_{r1}\) & \(n_{r2}\) & \(\cdots\) & \(n_{rs}\) & \(a_r\)
        \\ \midrule
        Total & \(b_1\) & \(b_2\) & \(\cdots\) & \(b_s\) & {}
    \end{tabular}
    \caption{The contingency table for two clusterings \(U, V\) of a dataset
    \textbf{X}.}\label{tab:contingency}
    \end{table}

    In addition to these entries, we have taken the sums of the each row and of
    each column, and denoted them by \(a\)~and~\(b\), respectively. That is:
    \[
        a_i = \sum_{j=1}^s n_{ij} \ \forall i = 1, \ldots, r, \quad \text{and}
        \quad b_j = \sum_{i=1}^r n_{ij} \ \forall j = 1, \ldots, s
    \]\\
\end{definition}

\begin{definition}\label{def:adjusted-rand-index}
    Let \textbf{X} be a dataset, \(U, V\) be two clusterings of the elements of
    \textbf{X}, and \(\left[n_{ij}\right]\) be the corresponding contingency
    table. Then the adjusted Rand index of these clusterings is given by:
    \[
        ARI(U, V) = \frac{\displaystyle{\sum_{ij} {n_{ij}\choose 2} -
        \frac{1}{{N\choose 2}}\left[\sum_i {a_i\choose 2}\sum_j {b_j\choose
        2}\right]}}{\displaystyle{\frac{1}{2} \left[\sum_i {a_i\choose 2} +
        \sum_j{b_j\choose 2}\right] - \frac{1}{{N\choose 2}}\left[\sum_i
        {a_i\choose 2}\sum_j {b_j\choose 2}\right]}}
    \]\\
\end{definition}

\begin{definition}\label{def:adjusted-mutual-info}
    Let \textbf{X} be a dataset, \(U, V\) be two clusterings of the elements of
    \textbf{X}, and \(\left[n_{ij}\right]\) be the corresponding contingency
    table. Then we define the following quantities:
    \begin{itemize}
        \item Consider a clustering \(C = \left\{C_1, \ldots, C_k\right\}\) of
            \textbf{X}. Then the \emph{entropy}, denoted by \(H(C)\), which is
            associated with this clustering is defined to be:
            \[
                H(C) = - \sum_{i=1}^k P(C_i)\log P(C_i), \quad \text{where}
                \quad P(C_i) = \frac{\left|C_i\right|}{N}
            \]

            \(H(C)\) is non-negative and only takes the value \(0\) when
            \(k~=~1\), i.e.\ there is only one cluster.

        \item Consider our two clusterings \(U, V\). We wish to quantify the
            information shared by both clusterings. This quantity is called the
            \emph{mutual information} between these clusterings, and is defined
            to be:
            \[
                MI(U, V) = \sum_{i=1}^r \sum_{j=1}^s P(U_i, V_j)\log
                \frac{P(U_i, V_j)}{P(U_i)P(V_j)}, \quad \text{where} \quad P(U_i,
                V_j) = \frac{\left|U_i \cap V_j\right|}{N}
            \]

            \(MI(U, V)\) is also non-negative and is bounded from above by
            \(H(U)\) and \(H(V)\).

        \item Using a combinatorial approach (in a similar fashion to
            Definition~\ref{def:adjusted-rand-index}), and a hypergeometric
            model of randomness, we define the
            \emph{expected~mutual~information} of two random clusterings to be:
            \[
                \mathbb{E}\left(MI(U, V)\right) = \sum_{i=1}^r \sum_{j=1}^s
                \sum_{n_{ij}=n_{ij}^*}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log
                \left(\frac{n_{ij} N}{a_i b_j}\right) \times \frac{a_i!\, b_j!\,
                \left(N - a_i\right)!\, \left(N - b_j\right)!}{N!\, n_{ij}!\,
                \left(a_i - n_{ij}\right)!\, \left(b_j - n_{ij}\right)!\,
                \left(N - a_i - b_j + n_{ij}\right)!}
            \]
            Here, we have \(n_{ij}^* = \max(1, a_i + b_j - N)\).\\
    \end{itemize}
    
    With these quantities, we define the \emph{adjusted mutual information} of
    our pair of clusterings to be:
    \[
        AMI(U, V) = \frac{MI(U, V) - \mathbb{E}\left(MI(U, V)\right)}{\max
        \left(H(U), H(V)\right) - \mathbb{E}\left(MI(U, V)\right)}
    \]
\end{definition}

\begin{remark}
    This adjusted quantity takes values in the interval \(\left[0, 1\right]\).
    In fact, \(AMI(U, V)~=~1\) when \(U\) and \(V\) are indentical, and
    \(AMI(U, V)~=~0\) when the mutual information between the two clusterings is
    equal to the expectation of the mutual information, by chance.\\
\end{remark}

\begin{definition}\label{def:homogeneity}
\end{definition}

\begin{definition}\label{def:completeness}
\end{definition}

\subsection{The datasets}\label{subsec:datasets}

A bit on the structure of each dataset and links to access them.


\subsection{Results}\label{subsec:results}

Tables of results for each dataset and each initialisation process. Credit to 
\url{https://github.com/nicodv/kmodes} for the Python implementation of both the
Huang and Cao processes, as well as the $k$-modes algorithm itself.

\begin{table}[H]
\resizebox{\textwidth}{!}{%
\centering
    \input{tex/zoo-results.tex}
}
\end{table}
