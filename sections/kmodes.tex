\section{The \(k\)-modes algorithm}\label{sec:kmodes}

The following notation will be used throughout this work to describe the objects
associated with clustering a dataset:

\begin{itemize}
    \item Let \(\mathcal{A} := A_1 \times \cdots \times A_m\) denote the
        \emph{attribute space}. In this work, only categorical attributes are
        considered and so it is intuitive to describe each attribute as a set of
        its values, i.e.\ for each \(j = 1, \ldots, m\) it follows that \(A_j :=
        \left\{a_1^{(j)}, \ldots, a_{d_j}^{(j)}\right\}\) where \(d_j = |A_j|\)
        is considered the size of the \(j^{th}\) attribute.

    \item Let \(\mathcal{X} := \left\{X^{(1)}, \ldots, X^{(N)}\right\} \subset
        \mathcal{A}\) denote a \emph{dataset} where each \(X^{(i)} \in
        \mathcal{X}\) is defined as an \(m\)-tuple \(X^{(i)} := \left(x_1^{(i)},
        \ldots, x_m^{(i)}\right)\) where \(x_j^{(i)} \in A_j\) for each \(j = 1,
        \ldots, m\). The elements of \(\mathcal{X}\) are referred to as
        \emph{data points} or \emph{instances}. A dataset \(\mathcal{X}\) can be
        represented as a table like so:
        \begin{table}[H]
        \centering
        \begin{tabular}{cccccc}
            {} & \(A_1\) & \(A_2\) & \quad \ldots \quad & \(A_{m-1}\) & \(A_m\)
            \\
            \midrule
            \(X^{(1)}\) & \(x_1^{(1)}\) & \(x_2^{(1)}\) & \quad \ldots \quad & 
            \(x_{m-1}^{(1)}\) & \(x_m^{(1)}\)
            \\
            \(X^{(2)}\) & \(x_1^{(2)}\) & \(x_2^{(2)}\) & \quad \ldots \quad &
            \(x_{m-1}^{(2)}\) & \(x_m^{(2)}\)
            \\
            \vdots & \vdots & \vdots & {} & \vdots & \vdots
            \\
            \(X^{(N)}\) & \(x_1^{(N)}\) & \(x_2^{(N)}\) & \quad \ldots \quad &
            \(x_{m-1}^{(N)}\) & \(x_m^{(N)}\)
        \end{tabular}
        \end{table}

    \item Let \(\mathcal{Z} := \left(Z_1, \ldots, Z_k\right)\) be a partition
        of a dataset \(\mathcal{X}\) into \(k \in \mathbb{Z}^{+}\) distinct,
        non-empty parts. Such a partition \(\mathcal{Z}\) is called a
        \emph{clustering} of \(\mathcal{X}\).

    \item Each cluster \(Z_l\) has associated with it a \emph{representative
        point} (see Definition~\ref{def:mode}) which is denoted by \(z^{(l)} =
        \left(z_1^{(l)},~\ldots,~z_m^{(l)}\right) \in \mathcal{A}\).  These
        points may also be referred to as cluster modes. The set of all current
        representative points is denoted \(\overline Z = \left\{z^{(1)}, \ldots,
        z^{(k)}\right\}\).
\end{itemize}


An immediate difference between the \(k\)-means and \(k\)-modes algorithms is 
that they deal with different types of data, and so the metric used to define 
the distance between two points in our space must be different. With 
\(k\)-means, where the data has all-numeric attributes, Euclidean distance is 
often used. However, we do not have this sense of distance with categorical 
data. Instead, we utilise a dissimilarity measure \-- defined below \-- as our 
metric. It can be easily checked that this is indeed a distance measure.

\begin{definition}\label{def:dissim}
    Let \(\mathcal{X}\) be a dataset and consider any \(X^{(a)}, X^{(b)} \in
    \mathcal{X}\). The dissimilarity between \(X^{(a)}\) and \(X^{(b)}\),
    denoted by \(d\left(X^{(a)}, X^{(b)}\right)\), is given by:
    \begin{equation}
        d\left(X^{(a)}, X^{(b)}\right) := \sum_{j=1}^{m} \delta\left(x_j^{(a)},
        x_j^{(b)}\right) \quad \text{where} \quad \delta\left(x, y\right) =
        \begin{cases}
            0, & \text{if} \ x = y \\
            1, & \text{otherwise.}
        \end{cases}
    \end{equation}
    In other words, the dissimilarity between two points is the number of
    attributes where their values are not the same.
\end{definition}

%\input{tex/examples/dissim.tex}

Now that we have defined a metric on our space, we can turn our attention to
what we mean by the representative point \(\mu^{(l)}\) of a cluster \(C_l\). In
\(k\)-means, these representative points are called centroids or cluster centers
and are defined to be the average of all points \(X^{(i)} \in C_l\). With
categorical data, we use our revised distance measure from
Definition~\ref{def:dissim} to specify a representative point. We typically call
these points a mode of \(\mathcal{X}\).

\begin{definition}\label{def:mode}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset and consider some point
    \(z = \left(z_1, \ldots, z_m\right) \in \mathcal{A}\). Then \(z\) is called
    a \emph{mode} of \(\mathcal{X}\) if it minimises the following:
    \begin{equation}\label{eq:summed-dissim}
        D\left(\mathcal{X}, z\right) = \sum_{i=1}^{N} d\left(X^{(i)}, z\right)
    \end{equation}
\end{definition}

\begin{definition}\label{def:rel-freq}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset. Then
    \(n\left(a_s^{(j)}\right)\) denotes the \emph{frequency} of the \(s^{th}\)
    category \(a_s^{(j)}\) of \(A_j\) in \(\mathcal{X}\), i.e.\ for each \(A_j
    \in \mathcal{A}\) and each \(s = 1, \ldots, d_j\):
    \begin{equation}
        n\left(a_s^{(j)}\right) := \abs*{%
            {\left\{X^{(i)} \in \mathcal{X}: x_j^{(i)} = a_s^{(j)}\right\}}
        }
    \end{equation}
	
    Furthermore, \(\frac{n\left(a_s^{(j)}\right)}{N}\) is called the
    \emph{relative frequency} of category \(a_s^{(j)}\) in \(\mathcal{X}\).
\end{definition}

\begin{theorem}\label{thm:1}
    Consider a dataset \(\mathcal{X} \subset \mathcal{A}\) and some \(U = (u_1,
    \ldots, u_m) \in \mathcal{A}\). Then \(D(\mathcal{X}, U)\) is minimised if
    and only if \(n\left(u_j\right) \geq n\left(a_s^{(j)}\right)\) for all
    \(s=1, \ldots, d_j\) for each \(j = 1, \ldots, m\).

    A proof of this theorem can be found in the Appendix of~\cite{Huang1998}.
\end{theorem}

%\input{tex/examples/mode.tex}

\begin{definition}\label{def:cost}
    Let \(\mathcal{Z} = \left\{Z_1, \ldots, Z_k\right\}\) be a clustering of a
    dataset \(\mathcal{X}\), and let \(\overline Z = \left\{z^{(1)},
    \ldots, z^{(k)}\right\}\) be the corresponding cluster modes. Then \(W =
    \left(w_{i, l}\right)\) is an \(N \times k\) \emph{partition matrix} of
    \(\mathcal{X}\) such that:
    \[
        w_{i, l} = \begin{cases}
                     1, & \text{if} \ X^{(i)} \in Z_l\\
                     0, & \text{otherwise.}
                   \end{cases}
    \]

    With this, the \emph{cost function} is defined to be the summed
    within-cluster dissimilarity:
    \begin{equation}
        C\left(W, \overline Z\right) := \sum_{l=1}^{k} \sum_{i=1}^{N}
        \sum_{j=1}^{m} w_{i,l} \ \delta\left(x_j^{(i)}, z_j^{(l)}\right)
    \end{equation}
\end{definition}

Below is a practical implementation of the \(k\)-modes
algorithm~\cite{Huang1998}:

\input{tex/algorithms/kmodes.tex}

\begin{remark}
    The processes by which the \(k\) initial modes are selected are detailed in 
    Sections~\ref{sec:init}~\&~\ref{sec:proposed-method}.
\end{remark}
