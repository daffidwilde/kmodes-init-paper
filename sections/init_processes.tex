\section{Initialisation processes}\label{sec:init}

From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Cao2009, Huang1998}. While some works attempt to improve the
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng2007}, this work will examine the way in which
these \(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsec:huang}~\&~\ref{subsec:cao}.


\subsection{Huang's method}\label{subsec:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described 
in~\cite{Huang1998}. Here, we consider two sets of modes, \(\tilde{\mu}\) and
\(\bar{\mu}\). The former acts as a placeholder set of modes, whereas the latter
is the set of modes to go on to be used by the \(k\)-modes algorithm.

\begin{singlespace}
    \input{tex/algorithms/huang.tex}
\end{singlespace}

In the original statement of Huang's method, the algorithm states that the most
frequent categories should be assigned `equally' to the \(k\) initial modes. How
the categories should be distributed `equally' is not well-defined or easily
seen from the example given. This ambiguity in the definition of Huang's method
means that a probabilistic element must be introduced, and unless seeded
pseudo-random numbers are used, computer-generated results are not necessarily
reproducible.

In our examples, and the implementation used to apply the \(k\)-modes algorithm
in Section~\ref{sec:results}, we will consider `equally' to mean taking a sample
from a probability distribution. This distribution will be formed by the
relative frequencies of the attributes' values, as is described in
Algorithm~\ref{alg:huang}.

\begin{example}\label{ex:huang}
    Consider our vehicle dataset. We will now find a set of initial modes for
    the \(k\)-modes algorithm using Huang's method. For the sake of this
    example, we will let \(k = 3\).
    
    \begin{table}[H]
    \centering
    \singlespacing{%
    \resizebox{.8\textwidth}{!}{%
        \input{tex/relative_freq.tex}
    }}
    \caption{Relative frequency table for attribute values.}\label{tab:rel-freq}
    \end{table}

    We begin by calculating the relative frequencies of our attributes' values,
    which are stored in Table~\ref{tab:rel-freq}. Now to find our set of
    (potentially) virtual modes, \(\tilde{\mu}\). For each attribute, we will
    take a sample of size one from the its set of values according to the
    probability distribution represented in the corresponding column of
    Table~\ref{tab:rel-freq}. Let us begin with the first attribute of our first
    mode in \(\tilde{\mu}\). Then we have to sample from the following
    probability distribution:
    
    \begin{table}[H]
    \centering
    \singlespacing{%
    \begin{tabular}{cccccc}
        \(A_{1}\) &\vline& L & M & H & V \\
        \midrule\(\mathbb{P}(A_{1} = a_s^{(1)})\) &\vline& \(\frac{2}{10}\) &
        \(\frac{2}{10}\) & \(\frac{4}{10}\) & \(\frac{2}{10}\) 
    \end{tabular}
    }
    \end{table}
    
    We sample one value from this distribution and set that value to be the
    first component of our first mode. This process is repeated for all values
    \(l = 1, 2, 3\) and \(j = 1, \ldots, 6\) giving us \(3\) \(m\)-dimensional
    vectors that fairly represent the most frequent attribute values. This set
    of vectors is \(\tilde{\mu}\).

    There are many ways of obtaining \(\tilde{\mu}\) from our relative frequency
    table but we have opted to do so using a short Python script (see Appendix).
    In this case, we have the following set of vectors:

    \input{tex/huang_virtual_modes.tex}

    Finally, we take each element of \(\tilde{\mu}\) in turn and find its most
    similar point in the dataset. This collection of points in the dataset then
    forms our set of initial modes \(\bar{\mu}\) to be passed on to the 
    \(k\)-modes algorithm. We stipulate that no point which is identical to
    another that has been already selected may be used as an initial mode. This
    is done so as to avoid empty clusters further down the line.

    \begin{table}[H]
    \centering
    \singlespacing{%
    \resizebox{.8\textwidth}{!}{%
        \input{tex/huang_dissim.tex}
    }}
    \caption{The dataset ranked by dissimilarity to the first element of
    \(\tilde{\mu}\).}\label{tab:huang-mode-dissim}
    \end{table}

    Taking the first element of \(\tilde{\mu}\), we calculate the dissimilarity
    between this vector and all of our datapoints.
    Table~\ref{tab:huang-mode-dissim} shows the elements of our dataset ranked
    in ascending order of their dissimilarity to this vector. It follows that we
    should set our first initial mode to be the sixth entry of our dataset. We
    continue this process for the other elements of \(\tilde{\mu}\),
    disregarding any points that have already been selected.
    
    Using our Python implementation for Huang's initialisation method we have 
    that the set of initial modes for this instance of the \(k\)-modes algorithm
    correspond to the sixth, fifth and fourth rows of the dataset. That is, we
    have:

    \input{tex/huang_initial_modes.tex}
\end{example}

\begin{remark}
    In practice, taking a random sample according to some probability
    distribution will lead to variation of results. For instance, in the example
    above, the only true mode of the dataset was not selected as an initial
    mode. This is due to random sampling. As such, when Huang's method is used
    to initialise the \(k\)-modes algorithm it is typically ran multiple times
    and the result with lowest final cost is used.
\end{remark}

\subsection{Cao's method}\label{subsec:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic
element \- unlike Huang's method or a random initialisation. So, we can consider
the results to be largely reproducible, except in the case where a tie must be
broken (see Example~\ref{ex:cao}).

\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao2009} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \quad \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]

    Observe that:
    \[
	    |\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	    \sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \]

    And so, we can find an alternative definition for \(\text{Dens}(X^{(i)})\):
    \begin{equation}\label{eq:alt-def}
    \begin{aligned}
        \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N (1
        - \delta(x_j^{(i)}, x_j^{(t)}))
        \\
        = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - \frac{1}{mN}
        \sum_{j=1}^m \sum_{t=1}^N \delta(x_j^{(i)}, x_j^{(t)})
        \\
        = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N d(X^{(i)}, X^{(t)})
        \\
        = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
    \end{aligned}
    \end{equation}
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since:		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\) for all \(j = 1, \ldots, m\), then
            \(\text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \frac{1}{N}}{m} =
            \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N\) for all \(j = 1, \ldots, m\), then
            \(\text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} =
            1\)
	\end{itemize}
\end{remark}

\begin{singlespace}
    \input{tex/algorithms/cao.tex}
\end{singlespace}

\begin{remark}
    With this alternative definition, we see \-- since \(m\) and \(N\) are fixed
    positive integers \-- that \(\text{Dens}(X^{(i)})\) is maximised when
    \(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem~\ref{thm:1} we have
    that such an \(X^{(i)}\) with maximal average density in \textbf{X} with
    respect to \textbf{A} is, in fact, a mode of \textbf{X}. This observation
    allows us to consider some sense of similarity between Huang and Cao's
    methods, as they seem to be trying to achieve the same objective \-- if only
    from opposite ends.
\end{remark}

\begin{example}\label{ex:cao}
    We will now attempt to find \(3\) initial modes for our vehicle dataset 
    using Cao's method, as we did in Example~\ref{ex:huang}. We begin by 
    calculating the average density of each of our points. We rank these in 
    descending order, and take the point with maximal density as our first 
    initial mode. This ranking is shown in Table~\ref{tab:ranked-density}.

    \begin{table}[H]
        \centering
        \singlespacing{%
        \resizebox{.8\textwidth}{!}{%
            \input{tex/ranked_density_table.tex}
        }}
        \caption{The dataset ranked by average
            density.}\label{tab:ranked-density}
    \end{table}

    So, from Table~\ref{tab:ranked-density}, we see that the first row should be
    taken as our first initial mode, \(\mu^{(1)}\). This is something we should
    expect since it was seen in Example~\ref{ex:mode} that this entry has
    minimal summed dissimilarity, and from Equation~\ref{eq:alt-def} we know
    that this is equivalent to maximising density.
    
    Now, we wish to find the point which has the maximal product of its density
    and its dissimilarity with our first mode. One way of doing this is to 
    calculate the dissimilarity between each point and the mode, append this
    as a column to our table and multiply these two new columns by each other
    to give \(\text{Dens}(X^{(i)}) \times d(\mu^{(1)}, X^{(i)})\) for each \(i =
    1, \ldots, 10\). The entries are then ranked by this product, and the first
    entry is taken as the second mode. By inspecting 
    Table~\ref{tab:ranked-dens-dissim}, we see that there is a tie. In practical
    implementations we can only assume that ties are broken arbitrarily. So, we
    shall take the fourth row as our second initial mode, \(\mu^{(2)}\).

    \begin{table}[H]
        \singlespacing{%
        \resizebox{\textwidth}{!}{%
            \input{tex/ranked_dens_dissim_table.tex}
        }}
        \caption{A ranking of the dataset by those who have highest
            density-dissimilarity product with the first
            mode.}\label{tab:ranked-dens-dissim}
    \end{table}

    In order to find the final initial mode, \(\mu^{(3)}\), we actually need to
    find a pair \((X^{(i_3)}, \mu^{(m)})\) as is stated in 
    Algorithm~\ref{alg:cao}. In order to do this, and the process would be the
    same for any further modes, we must consider all of our current initial
    modes, the dissimilarity between each point in our dataset and these modes,
    and the density of each point in the dataset. A convenient way of displaying
    all of this information is to construct a density-dissimilarity matrix which
    we denote by \(\mathbb{D}\) and define as follows:
    \begin{itemize}
        \item \(\mathbb{D}\) has \(|\bar{\mu}|\) rows and \(N\) columns, where
            \(|\bar{\mu}|\) is the number of initial modes already selected.
        \item The entries of \(\mathbb{D}\) are given by:
            \[
                \mathbb{D}_{li} = \text{Dens}(X^{(i)}) \times d(X^{(i)},
                \mu^{(l)}) \ \text{for all} \ l = 1, \ldots, |\bar{\mu}| \
                \text{and} \ i = 1, \ldots, N
            \]
    \end{itemize}

    Now, we go through each column and highlight the smallest value. These
    represent which current mode has minimal density-dissimilarity with the
    \(i^{th}\) datapoint (column). Then, we go through the highlighted entries
    and select the column which has the largest value. This column corresponds
    to the next datapoint to be selected as an initial mode. This process is
    shown in Figure~\ref{fig:cao-matrix}.
    
    \begin{figure}[H]
        \centering
        \singlespacing{%
        \begin{minipage}{\textwidth}
            \centering
            \(
            \begin{pmatrix}
                0 & 1.2 & 1.1\dot{3} & 1.5\dot{3} & 1.3 & 1.4\dot{6} &
                1.5\dot{3} & 1.15 & 0.95 & 0.8\dot{6}
                \\
                1.9\dot{3} & 1.8 & 1.7 & 0 & 1.3 & 1.1 & 1.15 & 1.91\dot{6} &
                1.2\dot{6} & 1.3
            \end{pmatrix}
            \)
        \end{minipage}

        \vspace{10pt}

        \begin{minipage}{\textwidth}
            \centering
            \(
            \begin{pmatrix}
                \underline{0} & \underline{1.2} &
                \underline{1.1\dot{3}} & 1.5\dot{3} & \underline{1.3}
                & 1.4\dot{6} & 1.5\dot{3} & \underline{1.15} &
                \underline{0.95} & \underline{0.8\dot{6}}
                \\
                1.9\dot{3} & 1.8 & 1.7 & \underline{0} &
                \underline{1.3} & \underline{1.1} &
                \underline{1.15} & 1.91\dot{6} & 1.2\dot{6} & 1.3
            \end{pmatrix}
            \)
        \end{minipage}

        \vspace{10pt}

        \begin{minipage}{\textwidth}
            \centering
            \(
            \begin{pmatrix}
                \underline{0} & \underline{1.2} & \underline{1.1\dot{3}} &
                1.5\dot{3} & \textcolor{red}{\underline{1.3}} & 1.4\dot{6} &
                1.5\dot{3} & \underline{1.15} & \underline{0.95} &
                \underline{0.8\dot{6}}
                \\
                1.9\dot{3} & 1.8 & 1.7 & \underline{0} &
                \textcolor{red}{\underline{1.3}} & \underline{1.1} &
                \underline{1.15} & 1.91\dot{6} & 1.2\dot{6} & 1.3
            \end{pmatrix}
            \)
        \end{minipage}
        }
        \caption{The stages of selecting the \(l^{th}\) mode with a
        density-dissimilarity matrix, for \(l > 2\). First, the row with smaller
        value is highlighted in each column (underlined here). Then of those
        highlighted entries, the entry with maximal value is selected (shown in
    red). Ties are broken arbitrarily.}\label{fig:cao-matrix}
    \end{figure}

    Therefore, our set of initial modes, \(\bar{\mu}\), correspond to the first,
    fourth and fifth rows of our dataset. That is:
    
    \input{tex/cao_initial_modes.tex}
\end{example}
