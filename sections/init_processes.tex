\section{Initialisation processes}\label{sec:init}

From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Cao2009, Huang1998}. While some works attempt to improve the
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng2007}, this work will examine the way in which
these \(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsec:huang}~\&~\ref{subsec:cao}.


\subsection{Huang's method}\label{subsec:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described 
in~\cite{Huang1998}. Here, we consider two sets of modes, \(\tilde{\mu}\) and
\(\bar{\mu}\). The former acts as a placeholder set of modes, whereas the latter
is the set of modes to go on to be used by the \(k\)-modes algorithm.

\begin{singlespace}
    \input{tex/algorithms/huang.tex}
\end{singlespace}

In the original statement of Huang's method, the algorithm states that the most
frequent categories should be assigned `equally' to the \(k\) initial modes. How
the categories should be distributed `equally' is not well-defined or easily
seen from the example given. This ambiguity in the definition of Huang's method
means that a probabilistic element must be introduced, and unless seeded
pseudo-random numbers are used, computer-generated results are not necessarily
reproducible.

In this work, as is done in the implementation used to apply the \(k\)-modes
algorithm in Section~\ref{sec:results}, the term `equally' is considered to mean
taking a sample from a probability distribution. This distribution is formed by
the relative frequencies of the attributes' values (defined in
Definition~\ref{def:rel-freq}), as is described in Algorithm~\ref{alg:huang}.

In practice, taking a random sample according to some probability distribution
will lead to variation between runs of this method. As such, when Huang's method
is used to initialise the \(k\)-modes algorithm it is typically run multiple
times and the result with lowest final cost is used.

%\input{tex/examples/huang.tex}

\subsection{Cao's method}\label{subsec:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic
element \- unlike Huang's method or a random initialisation. So, we can consider
the results to be largely reproducible, except in the case where a tie must be
broken (see Example~\ref{ex:cao}).

\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao2009} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \quad \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]

    Observe that:
    \[
	    |\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	    \sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \]

    And so, we can find an alternative definition for \(\text{Dens}(X^{(i)})\):
    \begin{equation}\label{eq:alt-def}
    \begin{aligned}
        \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N (1
        - \delta(x_j^{(i)}, x_j^{(t)}))
        \\
        = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - \frac{1}{mN}
        \sum_{j=1}^m \sum_{t=1}^N \delta(x_j^{(i)}, x_j^{(t)})
        \\
        = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N d(X^{(i)}, X^{(t)})
        \\
        = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
    \end{aligned}
    \end{equation}
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since:		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\) for all \(j = 1, \ldots, m\), then
            \(\text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \frac{1}{N}}{m} =
            \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N\) for all \(j = 1, \ldots, m\), then
            \(\text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} =
            1\)
	\end{itemize}
\end{remark}

\begin{singlespace}
    \input{tex/algorithms/cao.tex}
\end{singlespace}

\begin{remark}
    With this alternative definition, we see \-- since \(m\) and \(N\) are fixed
    positive integers \-- that \(\text{Dens}(X^{(i)})\) is maximised when
    \(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem~\ref{thm:1} we have
    that such an \(X^{(i)}\) with maximal average density in \textbf{X} with
    respect to \textbf{A} is, in fact, a mode of \textbf{X}. This observation
    allows us to consider some sense of similarity between Huang and Cao's
    methods, as they seem to be trying to achieve the same objective \-- if only
    from opposite ends.
\end{remark}

%\input{tex/examples/cao.tex}
