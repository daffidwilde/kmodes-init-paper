From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Huang98}~\cite{Cao09}. While some works attempt to improve the 
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng07}, this work will examine the way in which these
\(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsection:huang}~\&~\ref{subsection:cao}.


\subsection{Huang's method}\label{subsection:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described in 
\cite{Huang98}:

\begin{algorithm}[H]
\caption{Huang's method}\label{alg:huang}
    \begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \State{Let \(P = (p_{s,j})\) be an empty \(N \times m\) matrix.}
        \For{\(j = 1, \ldots, m\)}
            \State{\(d \gets |dom(A_j)|\)}
            \For{\(s = 1, \ldots, d\)}
                \State{Calculate \(n(a_s^{(j)})\).}
	        \EndFor
            \State{Sort \(dom(A_j) = \{a_1^{(j)}, \ldots, a_d^{(j)}\}\) into 
            descending order by frequency, breaking ties arbitrarily.}
            \State{Call this arrangement \(dom^*(A_j)\).}
            \For{\(a_s^{(j)} \in dom^*{A_j}\)}
                \State{\(p_{s,j} \gets \frac{n(a_s^{(j)})}{N}\)}
	        \EndFor
            \State{\(p_{s,j}\) is left empty for all \(s > d\).}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \For{\(j = 1, \ldots, m\)}
                \State{Take the nonempty entries of \(p_{*,j}\) as a vector and 
                consider it as a probability distribution.}
            \State{Sample \(a_{s^*}^{(j)}\) from \(dom^*(A_j)\) according to 
            this probability distribution.}
            \State{\(\mu_j^{(l)} \gets a_{s^*}^{(j)}\)}
	        \EndFor
            \State{\(\bar{\mu} \gets \bar{\mu} \cup \mu^{(l)}\)}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \State{Select \(X^{(i^*)} \in \textbf{X}\) such that: 
            \[
                d(X^{(i^*)}, \mu^{(l)}) = \min_{1 \le i \le  N} 
                \{d(X^{(i)}, \mu^{(l)})\} \text{ and } X^{(i^*)} \ne \mu^{(l')}
                \text{ for all } \mu^{(l')} \in \bar{\mu}
            \]
            }
            \State{\(\mu^{(l)} \gets X^{(i^*)}\)}
            \State{\(\bar{\mu} \gets \mu^{(l)}\)}
	    \EndFor
    \end{algorithmic}
\end{algorithm} 

In the original statement of Huang's method~\cite{Huang98}, the algorithm states
that the most frequent categories should be assigned `equally' to the $k$ 
initial modes. How the categories should be distributed `equally' is not 
well-defined or easily seen from the example given. In 
Section~\ref{section:results}, an implementation of the $k$-modes algorithm 
(written in Python) is used to compare the quality of the initialisation 
processes discussed throughout this piece of work when applied to a collection 
of datasets. That 
implementation distributes the attribute values in a random way (with replacement) according to the probability distribution formed by the relative frequencies of each category for each attribute. \\ In our examples, we will assign the categories to our initial modes in the same way, as it is described in Algorithm \ref{alg:huang}. This ambiguity in the definition of Huang's method means that a probabilistic element must be introduced, and unless seeded pseudo-random numbers are used, results are not necessarily reproducible. \\ A small example of this method is given below. \\ {\textbf{\large{Skip this example and replace with the toy example}}} \begin{example}	Below are the first five rows of a random sample of 250 records from a data set used to determine the acceptability of a car. This dataset was chosen primarily for its number of attributes. However, it should be noted that one downfall of this particular data set is that some of the attributes could be considered as ordinal rather than purely categorical since there are clearly established and easily understandable differences between "high" and "low" prices, for instance. \\ \begin{table}[H] \centering \begin{tabular}{c|c|c|c|c|c}\label{table:1} Price & Maintenance & Doors & Passengers & Luggage & Safety \\ \hline low &               vhigh &            2 &                  5+ &              med &          med \\ vhigh &             high &            2 &                  4 &              big &          med \\ high &             med &            2 &                  2 &              small &          low \\          vhigh &              med &            3 &                  2 &            big &          low \\         low &             med &            5+ &                  2 &              big &           low \\ \end{tabular} \end{table} The frequencies of our attributes' categories are given below: \\ \begin{table}[H] \centering \begin{tabular}{c|c|c|c|c|c}\label{table:2} Price	&	Maintenance	&	Doors	&	Passenger	&	Luggage &	Safety	\\ \hline $ f(c_{\text{low}}) = 61 $		&	$ f(c_{\text{low}}) = 53 $		&	$ f(c_{2}) = 71 $	   	&	$ f(c_{2}) = 81 $		&	$ f(c_{\text{small}}) = 88 $		&	$ f(c_{\text{low}}) = 76 $	\\ $ f(c_{\text{med}}) = 63 $		&	$ f(c_{\text{med}}) = 66 $		&	$ f(c_{3}) = 71 $		& 	$ f(c_{4}) = 85 $		&	$ f(c_{\text{med}}) = 78 $	&	$ f(c_{\text{med}}) = 91 $	\\ $ f(c_{\text{high}}) = 63 $		 &	 $ f(c_{\text{high}}) = 50 $		  &	  $ f(c_{4}) = 53 $	   &	$ f(c_{5+}) = 84 $	&	$ f(c_{\text{big}}) = 84 $	  &		$ f(c_{\text{high}}) = 83 $	\\ $ f(c_{\text{vhigh}}) = 63 $	&	$ f(c_{\text{vhigh}}) = 81 $	&	$ f(c_{5+}) = 55 $		&				{}					&					{}						&						{}					\\ \end{tabular} \caption{Frequencies of all attribute categories, $f(c_{s,j})$} \end{table}

	Thus, from Table \ref{table:2} we see that our category matrix is:
	
	\[
	\begin{pmatrix}		
		\text{vhigh}	&	\text{vhigh}	&	3	&	4	&	\text{small}	&	\text{med}	\\
		\text{high}		&	\text{med}		&	2	&	5+	&	\text{big}	&	\text{high}		\\
		\text{med}		&	\text{low}		&	5+	&	2	&	\text{med}	&	\text{low}		\\
		\text{low}		&	\text{high}		&	4	&	{}	&			{}			&			{}			\\
	\end{pmatrix}
	\] \\
	
	
	Acceptability is an attribute of this data which has been removed but indicates whether a car is one of `very good', `good', `acceptable' or `unacceptable'. From this we can suppose that we are looking for $k = 4$ clusters, and so, by distributing the most frequent categories `equally' our initial set of modes is:
	
	\begin{equation}
	\begin{aligned}
	\bar{\mu} = & \{\mu^{(1)} = [\text{vhigh}, \text{med}, 5+, 4, \text{big}, \text{low}], \ \ \mu^{(2)} = [\text{high}, \text{low}, 4, 5+, \text{med}, \text{med}], \\
						  & \ \ \mu^{(3)} = [\text{med}, \text{high}, 3, 2, \text{small}, \text{high}], \ \ \mu^{(4)} = [\text{low}, \text{vhigh}, 2, 4, \text{big}, \text{med}] \} \\
	\end{aligned}
	\end{equation} \\
	
	Now we would select the least dissimilar point in our data set to replace each $\mu^{(l)} \in \bar{\mu}$ in numerical order according to Definition \ref{def:dissim} and continue with the rest of the algorithm. \\
\end{example}


\subsection{Cao's method}\label{subsection:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic 
element - unlike the standard or Huang's method - and so results are completely
reproducible.


\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao09} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since for any \(A_j \in \textbf{A}\):		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\), then \(\text{Dens}(X^{(i)}) = 
			\frac{\sum_{j=1}^m \frac{1}{N}}{m} = \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N$, then $\text{Dens}(X^{(i)}) = 
            \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} = 1\)
	\end{itemize}
\end{remark}
    
Observe that:
\[
	|\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	\sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
\]\\

and so, we can find an alternative definition for $\text{Dens}(X^{(i)})$:
\begin{equation}
\begin{aligned}
    \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \\
			             = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - 
                         \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         \delta(x_j^{(i)}, x_j^{(t)})
    \\
                         = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N 
                         d(X^{(i)}, X^{(t)})
    \\
			             = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
\end{aligned}
\end{equation}\\

With this alternative definition, we see - since \(m\) and \(N\) are fixed 
positive integers - that \(\text{Dens}(X^{(i)})\) is maximised when 
\(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem \ref{theorem:1} we have
that such an \(X^{(i)}\) with maximal average density in \textbf{X} with respect
to \textbf{A} is, in fact, a mode of \textbf{X}. This notion helps justify the 
method proposed by Cao et al. as discussed below.\\

\begin{algorithm}[H]
\caption{Cao's method}\label{alg:cao}
	\begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \For{\(X^{(i)} \in \textbf{X}\)}
            \State{Calculate \(\text{Dens}(X^{(i)})\).}
		\EndFor
        \State{Select \(X^{(i_1)} \in \textbf{X}\) which satisfies:
        \[
            \text{Dens}(X^{(i_1)}) = \max_{X^{(i)} \in \textbf{X}} 
            \{\text{Dens}(X^{(i)})\}
        \]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_1)}\)}
        \State{Select \(X^{(i_2)} \in \textbf{X}\) such that: 
		\[
			\text{Dens}(X^{(i_2)}) \times d(X^{(i_1)}, X^{(i_2)}) =
			\max_{X^{(i)} \in \textbf{X}} \{d(X^{(i)}, X^{(i_1)})\}
		\]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_2)}\)}
        \While{\(|\bar{\mu}| < k\)}
            \State{Select \(X^{(i_t)} \in \textbf{X}\) such that for all 
            \(\mu^{(l)} \in \bar{\mu}\):
			\[
		        d(X^{(i_t)}, \mu^{(l)}) \times \text{Dens}(X_{i_t}) = 
                \max_{X^{(i)} \in \textbf{X}} \{\min_{\mu^{(l)} \in \bar{\mu}} 
				\{d(X^{i}, \mu^{(l)}) \times \text{Dens}(X^{(i)}) \}\}
			\]
            }
            \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_t)}\)}
		\EndWhile
	\end{algorithmic}
\end{algorithm}

