From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Huang98}~\cite{Cao09}. While some works attempt to improve the 
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng07}, this work will examine the way in which these
\(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsec:huang}~\&~\ref{subsec:cao}.\\


\subsection{Huang's method}\label{subsec:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described 
in~\cite{Huang98}. Here, we consider two sets of modes, \(\tilde{\mu}\) and
\(\bar{\mu}\). The former acts as a placeholder set of modes, whereas the latter
is the set of modes to go on to be used by the \(k\)-modes algorithm.\\

\input{tex/algorithms/huang.tex} 

In the original statement of Huang's method~\cite{Huang98}, the algorithm states
that the most frequent categories should be assigned `equally' to the \(k\) 
initial modes. How the categories should be distributed `equally' is not 
well-defined or easily seen from the example given. This ambiguity in the 
definition of Huang's method means that a probabilistic element must be 
introduced, and unless seeded pseudo-random numbers are used, computer-generated
results are not necessarily reproducible.\\

In our examples, and the implementation used to apply the \(k\)-modes algorithm
in Section~\ref{sec:results}, we will consider `equally' to mean taking a sample
from a probability distribution. This distribution will be formed by the
relative frequencies of the attributes' values, as is described in
Algorithm~\ref{alg:huang}.\\

\begin{example}\label{ex:huang}
    Consider our vehicle dataset. We will now find a set of initial modes for
    the \(k\)-modes algorithm using Huang's method. For the sake of this
    example, we will let \(k = 3\).\\
    
    \begin{table}[H]
    \centering
    \resizebox{.8\textwidth}{!}{%
        \input{tex/rel-freq-table.tex}
    }
    \caption{Relative frequency table for attribute values.}\label{tab:rel-freq}
    \end{table}

    We begin by calculating the relative frequencies of our attributes' values,
    which are stored in Table~\ref{tab:rel-freq}. Now to find our set of
    (potentially) virtual modes, \(\tilde{\mu}\). For each attribute, we will
    take a sample of size one from the its set of values according to the
    probability distribution represented in the corresponding column of
    Table~\ref{tab:rel-freq}. Let us begin with the first attribute of our first
    mode in \(\tilde{\mu}\). Then we have to sample from the following
    probability distribution:\\
    
    \begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \(A_{1}\) &\vline& L & M & H & V \\
        \midrule\(\mathbb{P}(A_{1} = a_s^{(1)})\) &\vline& \(\frac{2}{10}\) &
        \(\frac{2}{10}\) & \(\frac{4}{10}\) & \(\frac{2}{10}\)\\ 
    \end{tabular}
    \end{table}
    
    We sample one value from this distribution and set that value to be the
    first component of our first mode. This process is repeated for all values
    \(l = 1, 2, 3\) and \(j = 1, \ldots, 6\) giving us \(3\) \(m\)-dimensional
    vectors that fairly represent the most frequent attribute values. This set
    of vectors is \(\tilde{\mu}\).\\

    There are many ways of obtaining \(\tilde{\mu}\) from our relative frequency
    table but we have opted to do so using a short Python script (see Appendix).
    In this case, we have the following set of vectors:

    \input{tex/huang-tilde-mu.tex}

    Finally, we take each element of \(\tilde{\mu}\) in turn and find its most
    similar point in the dataset. This collection of points in the dataset then
    forms our set of initial modes \(\bar{\mu}\) to be passed on to the 
    \(k\)-modes algorithm. We stipulate that no point which is identical to
    another that has been already selected may be used as an initial mode. This
    is done so as to avoid empty clusters further down the line.\\

    \begin{table}[H]
    \centering
    \resizebox{.8\textwidth}{!}{%
        \input{tex/huang-mode-dissim.tex}
    }
    \caption{The dataset ranked by dissimilarity to the first element of
    \(\tilde{\mu}\).}\label{tab:huang-mode-dissim}
    \end{table}

    Taking the first element of \(\tilde{\mu}\), we calculate the dissimilarity
    between this vector and all of our datapoints. Table~\ref{huang-mode-dissim}
    shows the elements of our dataset ranked in ascending order of their
    dissimilarity to this vector. It follows that we should set our first
    initial mode to be the sixth entry of our dataset. We continue this process
    for the other elements of \(\tilde{\mu}\), disregarding any points that have
    already been selected.\\
    
    Using our Python implementation for Huang's initialisation method we have 
    that the set of initial modes for this instance of the \(k\)-modes algorithm
    correspond to the sixth, fifth and fourth rows of the dataset. That is, we
    have:

    \input{tex/huang-bar-mu.tex}
\end{example}

\begin{remark}
    In practice, taking a random sample according to some probability
    distribution will lead to variation of results. For instance, in the example
    above, the only true mode of the dataset was not selected as an initial
    mode. This is due to random sampling. As such, when Huang's method is used
    to initialise the \(k\)-modes algorithm it is typically ran multiple times
    and the result with lowest final cost is used.\\
\end{remark}

\subsection{Cao's method}\label{subsec:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic 
element - unlike the standard or Huang's method - and so results are completely
reproducible.\\

\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao09} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \quad \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]\\
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since for any \(A_j \in \textbf{A}\):		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\), then \(\text{Dens}(X^{(i)}) = 
			\frac{\sum_{j=1}^m \frac{1}{N}}{m} = \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N\), then \(\text{Dens}(X^{(i)}) = 
            \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} = 1\)\\
	\end{itemize}
\end{remark}

\noindent Observe that:
\[
	|\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	\sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
\]\\

and so, we can find an alternative definition for \(\text{Dens}(X^{(i)})\):
\begin{equation}
\begin{aligned}
    \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \\
			             = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - 
                         \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         \delta(x_j^{(i)}, x_j^{(t)})
    \\
                         = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N 
                         d(X^{(i)}, X^{(t)})
    \\
			             = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
\end{aligned}
\end{equation}\\

With this alternative definition, we see - since \(m\) and \(N\) are fixed 
positive integers - that \(\text{Dens}(X^{(i)})\) is maximised when 
\(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem \ref{thm:1} we have
that such an \(X^{(i)}\) with maximal average density in \textbf{X} with respect
to \textbf{A} is, in fact, a mode of \textbf{X}. This notion helps justify the 
method proposed by Cao et al. as discussed below.\\

\input{tex/algorithms/cao.tex}

\begin{example}\label{ex:cao}
    We will now attempt to find \(3\) initial modes for our zoo animal dataset 
    using Cao's method, as we did in Example~\ref{ex:huang}. We begin by 
    calculating the average density of each of our points. We rank these in 
    descending order, and take the point with maximal density as our first 
    initial mode. The top \(10\) most dense points in our dataset are shown in
    Table~\ref{tab:ranked-density}.

    \begin{table}[H]
    \resizebox{\textwidth}{!}{%
        \input{tex/ranked-density-table.tex}
    }
    \caption{The \(10\) points with highest average density.}\label{%
    tab:ranked-density}
    \end{table}

    So, from Table~\ref{tab:ranked-density}, we see that the tuatara should be
    taken as our first initial mode, \(\mu^{(1)}\). This is something we should
    expect since it was seen in Example~\ref{ex:mode} that the tuatara has
    minimal summed dissimilarity.\\
    
    Now, we wish to find the point which has the maximal product of its density
    and its dissimilarity with the tuatara. One way of doing this is to 
    calculate the dissimilarity between each point and the tuatara, append this
    as a column to our table and multiply these two new columns by each other
    to give \(\text{Dens}(X^{(i)}) \times d(X^{(92)}, X^{(i)})\) for each \(i =
    1, \ldots, 101\). The entries are then ranked by this product, and the first
    entry is taken as the second mode. By inspecting 
    Table~\ref{tab:ranked-dens-dissim}, we see that the seal should be set as
    the second initial mode, \(\mu^{(2)}\).\\

    \begin{table}[H]
    \resizebox{\textwidth}{!}{%
        \input{tex/ranked-dens-dissim-table.tex}
    }
    \caption{The \(10\) points which have highest density-dissimilarity product
    with the first mode.}\label{tab:ranked-dens-dissim}
    \end{table}

    In order to find the final initial mode, \(\mu^{(3)}\), we actually need to
    find a pair \((X^{(i_3)}, \mu^{(m)})\) as is stated in 
    Algorithm~\ref{alg:cao}. In order to do this, and if we needed to find any
    further modes, we must consider all of our current initial modes, the 
    dissimilarity between each point in our dataset and these modes, and the 
    density of each point in the dataset. A convenient way of displaying all of
    this information is to construct a density-dissimilarity matrix which we 
    denote by \(\mathbb{D}\) and define as follows:
    \begin{itemize}
        \item \(\mathbb{D}\) has \(|\bar{\mu}|\) rows and \(N\) columns, where
            \(|\bar{\mu}|\) is the number of initial modes already selected.
        \item The entries of \(\mathbb{D}\) are given by:
            \[
                \mathbb{D}_{li} = \text{Dens}(X^{(i)}) \times d(X^{(i)},
                \mu^{(l)}) \ \text{for all} \ l = 1, \ldots, |\bar{\mu}| \
                \text{and} \ i = 1, \ldots, N \\
            \]
    \end{itemize}

    Now, we go through each column and highlight the smallest value. These
    represent which current mode has minimal density-dissimilarity with the
    \(i^{th}\) datapoint (column). Then, we go through the highlighted entries
    and select the column which has the largest value. This column corresponds
    to the next datapoint to be selected as an initial mode.\\

    Therefore, our set of initial modes, \(\bar{\mu}\), correspond to the
    entries for the tuatara, the seal, and the honeybee.
\end{example}

