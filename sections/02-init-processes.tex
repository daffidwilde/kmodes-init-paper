From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Huang98}~\cite{Cao09}. While some works attempt to improve the 
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng07}, this work will examine the way in which these
\(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsec:huang}~\&~\ref{subsec:cao}.\\


\subsection{Huang's method}\label{subsec:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described 
in~\cite{Huang98}:\\

\begin{algorithm}[H]
\caption{Huang's method}\label{alg:huang}
    \begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \State{Let \(P = (p_{s,j})\) be an empty \(N \times m\) matrix.}
        \For{\(j = 1, \ldots, m\)}
            \State{\(d \gets |dom(A_j)|\)}
            \For{\(s = 1, \ldots, d\)}
                \State{Calculate \(n(a_s^{(j)})\).}
	        \EndFor
            \State{Sort \(dom(A_j) = \{a_1^{(j)}, \ldots, a_d^{(j)}\}\) into 
            descending order by frequency, breaking ties arbitrarily.}
            \State{Call this arrangement \(dom^*(A_j)\).}
            \For{\(a_s^{(j)} \in dom^*{A_j}\)}
                \State{\(p_{s,j} \gets \frac{n(a_s^{(j)})}{N}\)}
	        \EndFor
            \State{\(p_{s,j}\) is left empty for all \(s > d\).}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \For{\(j = 1, \ldots, m\)}
                \State{Take the nonempty entries of \(p_{*,j}\) as a vector and 
                consider it as a probability distribution.}
            \State{Sample \(a_{s^*}^{(j)}\) from \(dom^*(A_j)\) according to 
            this probability distribution.}
            \State{\(\mu_j^{(l)} \gets a_{s^*}^{(j)}\)}
	        \EndFor
            \State{\(\bar{\mu} \gets \bar{\mu} \cup \mu^{(l)}\)}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \State{Select \(X^{(i^*)} \in \textbf{X}\) such that: 
            \[
                d(X^{(i^*)}, \mu^{(l)}) = \min_{1 \le i \le  N} 
                \{d(X^{(i)}, \mu^{(l)})\} \text{ and } X^{(i^*)} \ne \mu^{(l')}
                \text{ for all } \mu^{(l')} \in \bar{\mu}
            \]
            }
            \State{\(\mu^{(l)} \gets X^{(i^*)}\)}
            \State{\(\bar{\mu} \gets \mu^{(l)}\)}
	    \EndFor
    \end{algorithmic}
\end{algorithm} 

In the original statement of Huang's method~\cite{Huang98}, the algorithm states
that the most frequent categories should be assigned `equally' to the \(k\) 
initial modes. How the categories should be distributed `equally' is not 
well-defined or easily seen from the example given. This ambiguity in the 
definition of Huang's method means that a probabilistic element must be 
introduced, and unless seeded pseudo-random numbers are used, computer-generated
results are not necessarily reproducible.\\

In our examples, and the implementation used to apply the \(k\)-modes algorithm
in Section~\ref{sec:results}, we will consider `equally' to mean taking a sample
from a probability distribution. This distribution will be formed by the
relative frequencies of the attributes' values, as is described in
Algorithm~\ref{alg:huang}.\\

\begin{example}\label{ex:huang}
    Consider the zoo animal dataset. We will now find a set of initial modes for
    the \(k\)-modes algorithm using Huang's method. For the sake of this
    example, we will let \(k = 3\). Be advised that this may not be the optimal
    value for \(k\).\\
    
    \begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \input{tex/rel-freq-frac-table.tex}
    }
    \caption{Relative frequency table for attribute values.}\label{tab:rel-freq}
    \end{table}

    We begin by calculating the relative frequencies of our attributes' values, 
    which are stored in Table~\ref{tab:rel-freq}. Now to find our set of 
    (potentially) virtual modes, \(\tilde{\mu}\). For each attribute, we will 
    take a sample of size one from the its set of values according to the 
    probability distribution represented in the corresponding column of 
    Table~\ref{tab:rel-freq}. For instance, say we are trying to find the value
    of the \(13^{th}\) attribute, `legs', of our first mode in \(\tilde{\mu}\).
    Then we have to sample from the following probability distribution:\\
    
    \begin{table}[h]
    \centering
    \begin{tabular}{cccccccc}
        \(A_{13}\) \vline& None & Two & Four & Five & Six & Eight \\
        \midrule\(\mathbb{P}(A_{13} = a_s^{13})\) \vline& \(\frac{23}{101}\) & 
        \(\frac{27}{101}\) & \(\frac{38}{101}\) & \(\frac{1}{101}\) & 
        \(\frac{10}{101}\) & \(\frac{2}{101}\) \\
    \end{tabular}
    \end{table}
    
    We sample one value from this distribution and set that value to be the 
    \(13^{th}\) component of our first mode. This process is repeated for all
    other \(l = 1, 2, 3\) and \(j = 1, \ldots, 16\) giving us \(3\)
    \(m\)-dimensional vectors that fairly represent the most frequent attribute
    values. This set of vectors is \(\tilde{\mu}\).\\

    There are many ways of obtaining \(\tilde{\mu}\) from our relative frequency
    table but we have opted to do so using a short Python script (see Appendix).
    In this case, we have the following set of vectors:

    \begin{equation}
    \nonumber
    \begin{aligned}
        \tilde{\mu} = \left\{some \ vectors \right\}\\
    \end{aligned}
    \end{equation}

    Finally, we take each element of \(\tilde{\mu}\) in turn and find its most
    similar point in the dataset so that no point is used twice. This collection
    of points in the dataset then forms our set of initial modes \(\bar{\mu}\)
    to be passed on to the \(k\)-modes algorithm. We also stipulate that (since
    there are duplicate rows in this dataset) no point which is identical to
    another which has been already selected may be used as an initial mode. This
    is done so as to avoid empty clusters further down the line.\\

    So, in the first case we have \(some \ vector\).

\end{example}

\subsection{Cao's method}\label{subsec:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic 
element - unlike the standard or Huang's method - and so results are completely
reproducible.\\

\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao09} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \quad \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]\\
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since for any \(A_j \in \textbf{A}\):		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\), then \(\text{Dens}(X^{(i)}) = 
			\frac{\sum_{j=1}^m \frac{1}{N}}{m} = \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N\), then \(\text{Dens}(X^{(i)}) = 
            \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} = 1\)\\
	\end{itemize}
\end{remark}

\noindent Observe that:
\[
	|\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	\sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
\]\\

and so, we can find an alternative definition for \(\text{Dens}(X^{(i)})\):
\begin{equation}
\begin{aligned}
    \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \\
			             = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - 
                         \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         \delta(x_j^{(i)}, x_j^{(t)})
    \\
                         = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N 
                         d(X^{(i)}, X^{(t)})
    \\
			             = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
\end{aligned}
\end{equation}\\

With this alternative definition, we see - since \(m\) and \(N\) are fixed 
positive integers - that \(\text{Dens}(X^{(i)})\) is maximised when 
\(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem \ref{thm:1} we have
that such an \(X^{(i)}\) with maximal average density in \textbf{X} with respect
to \textbf{A} is, in fact, a mode of \textbf{X}. This notion helps justify the 
method proposed by Cao et al. as discussed below.\\

\begin{algorithm}[H]
\caption{Cao's method}\label{alg:cao}
	\begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \For{\(X^{(i)} \in \textbf{X}\)}
            \State{Calculate \(\text{Dens}(X^{(i)})\).}
		\EndFor
        \State{Select \(X^{(i_1)} \in \textbf{X}\) which satisfies:
        \[
            \text{Dens}(X^{(i_1)}) = \max_{X^{(i)} \in \textbf{X}} 
            \{\text{Dens}(X^{(i)})\}
        \]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_1)}\)}
        \State{Select \(X^{(i_2)} \in \textbf{X}\) such that: 
		\[
			\text{Dens}(X^{(i_2)}) \times d(X^{(i_1)}, X^{(i_2)}) =
			\max_{X^{(i)} \in \textbf{X}} \{d(X^{(i)}, X^{(i_1)})\}
		\]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_2)}\)}
        \While{\(|\bar{\mu}| < k\)}
            \State{Select \(X^{(i_t)} \in \textbf{X}\) such that for all 
            \(\mu^{(l)} \in \bar{\mu}\):
			\[
		        d(X^{(i_t)}, \mu^{(l)}) \times \text{Dens}(X_{i_t}) = 
                \max_{X^{(i)} \in \textbf{X}} \{\min_{\mu^{(l)} \in \bar{\mu}} 
				\{d(X^{i}, \mu^{(l)}) \times \text{Dens}(X^{(i)}) \}\}
			\]
            }
            \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_t)}\)}
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{example}\label{ex:cao}
    Cao's method with zoo animal dataset
\end{example}

