From the literature surrounding this topic, it has been established that the 
initial choice of clusters impacts the final solution of the \(k\)-modes
algorithm~\cite{Huang98}~\cite{Cao09}. While some works attempt to improve the 
quality of \(k\)-modes and similar algorithms by considering an alternative 
dissimilarity measure~\cite{Ng07}, this work will examine the way in which these
\(k\) initial representative points are chosen. Two established methods of 
selecting these initial points are described in
Sections~\ref{subsec:huang}~\&~\ref{subsec:cao}.\\


\subsection{Huang's method}\label{subsec:huang}

In the standard form of the \(k\)-modes algorithm, the \(k\) initial modes are 
chosen at random from \textbf{X}. Below is an alternative method of selecting
these modes that forces some diversity between them, as described 
in~\cite{Huang98}:\\

\begin{algorithm}[H]
\caption{Huang's method}\label{alg:huang}
    \begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \State{Let \(P = (p_{s,j})\) be an empty \(N \times m\) matrix.}
        \For{\(j = 1, \ldots, m\)}
            \State{\(d \gets |dom(A_j)|\)}
            \For{\(s = 1, \ldots, d\)}
                \State{Calculate \(n(a_s^{(j)})\).}
	        \EndFor
            \State{Sort \(dom(A_j) = \{a_1^{(j)}, \ldots, a_d^{(j)}\}\) into 
            descending order by frequency, breaking ties arbitrarily.}
            \State{Call this arrangement \(dom^*(A_j)\).}
            \For{\(a_s^{(j)} \in dom^*{A_j}\)}
                \State{\(p_{s,j} \gets \frac{n(a_s^{(j)})}{N}\)}
	        \EndFor
            \State{\(p_{s,j}\) is left empty for all \(s > d\).}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \For{\(j = 1, \ldots, m\)}
                \State{Take the nonempty entries of \(p_{*,j}\) as a vector and 
                consider it as a probability distribution.}
            \State{Sample \(a_{s^*}^{(j)}\) from \(dom^*(A_j)\) according to 
            this probability distribution.}
            \State{\(\mu_j^{(l)} \gets a_{s^*}^{(j)}\)}
	        \EndFor
            \State{\(\bar{\mu} \gets \bar{\mu} \cup \mu^{(l)}\)}
	    \EndFor
        \For{\(l = 1, \ldots, k\)}
            \State{Select \(X^{(i^*)} \in \textbf{X}\) such that: 
            \[
                d(X^{(i^*)}, \mu^{(l)}) = \min_{1 \le i \le  N} 
                \{d(X^{(i)}, \mu^{(l)})\} \text{ and } X^{(i^*)} \ne \mu^{(l')}
                \text{ for all } \mu^{(l')} \in \bar{\mu}
            \]
            }
            \State{\(\mu^{(l)} \gets X^{(i^*)}\)}
            \State{\(\bar{\mu} \gets \mu^{(l)}\)}
	    \EndFor
    \end{algorithmic}
\end{algorithm} 

In the original statement of Huang's method~\cite{Huang98}, the algorithm states
that the most frequent categories should be assigned `equally' to the \(k\) 
initial modes. How the categories should be distributed `equally' is not 
well-defined or easily seen from the example given. This ambiguity in the 
definition of Huang's method means that a probabilistic element must be 
introduced, and unless seeded pseudo-random numbers are used, computer-generated results are not necessarily reproducible.\\

In Section~\ref{sec:results}, an implementation of the \(k\)-modes algorithm 
(written in Python) is used to compare the quality of the initialisation 
processes discussed throughout this piece of work when applied to a collection 
of datasets. That implementation distributes the attribute values by taking a
sample from each attribute domain (with replacement) according to the 
probability distribution formed by the relative frequencies of the attribute
values, as is described in Algorithm~\ref{alg:huang}. In our examples, we will
assign the categories to our initial modes in the same way.\\

\begin{example}\label{ex:huang}
    Consider the zoo animal dataset. We will now find a set of initial modes for
    the \(k\)-modes algorithm using Huang's method. For the sake of this
    example, we will let \(k = 7\) as there are \(7\) classes. Please be advised
    that this may not be the optimal value for \(k\).\\
    
    We begin by calculating the frequencies and relative frequencies of our 
    attributes' values, which are stored in 
    Tables~\ref{tab:freq}~\&~\ref{tab:rel-freq}, respectively.

    \begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \input{tex/frequency-table.tex}
    }
    \caption{Frequency table for attribute values.}\label{tab:freq}
    \end{table}

    Now that we have the frequency distributions for our attributes, we can
    arrange our attribute domains into descending order of frequency:
    
    \begin{equation}
    \begin{aligned}
    \nonumber
    \centering
        dom^*(\text{hair}) = \left[\false, \true\right], \
        dom^*(\text{feathers}) = \left[\false, \true \right], & \ &
        dom^*(\text{eggs}) = \left[\true, \false\right], \
        dom^*(\text{milk}) = \left[\false, \true\right],
        \\
        {} & \vdots & {}
        \\
        dom^*(\text{legs}) = \left[\text{Four, Two, None, Six, Eight, 
        Five}\right], \
        dom^*(\text{tail}) = \left[\true, \false\right], & \ &
        dom^*(\text{domestic}) = \left[\false, \true\right], \
        dom^*(\text{catsize}) = \left[\false, \true\right]
        \\
    \end{aligned}
    \end{equation}

    \begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \input{tex/rel-freq-frac-table.tex}
    }
    \caption{Relative frequency table for attribute values.}\label{tab:rel-freq}
    \end{table}

    Now to find our set of (potentially) virtual modes, \(\bar{\mu}\). For each
    attribute, we will take a sample of size one from the attribute domain
    according to the probability distribution represented in the corresponding
    column of Table~\ref{tab:rel-freq}. The following set of initial modes was 
    obtained by using a short Python script (put in Appendix). It can be easily
    checked that these vectors minimise our summed dissimilarity function
    \(D(\textbf{X}, \mu)\) with value \(101\), thus satisfying 
    Theorem~\ref{thm:1}.

    \begin{equation}
    \nonumber
    \begin{aligned}
        \bar{\mu} = \ \{ & \mu^{(1)} = \left[\false, \ \false, \ \true, \ 
        \false, \ \false, \ \true, \ \false, \ \true, \ \true, \ \true, \ 
        \false, \ \false, \ \text{Two}, \ \true, \ \false, \ \false\right],
        \\
        {} & \mu^{(2)} = \left[\false, \ \true, \ \true, \ \true, \ \true, \
        \true, \ \true, \ \true, \ \false, \ \true, \ \false, \ \true, \
        \text{Two}, \ \true, \ \false, \ \true\right],
        \\
        {} & \mu^{(3)} = \left[\false, \ \false, \ \false, \ \true, \ \false, \
        \false, \ \true, \ \true, \ \true, \ \true, \ \false, \false, 
        \text{None}, \ \true, \ \false, \ \false\right],
        \\
        {} & \mu^{(4)} = \left[\false, \ \false, \ \true, \ \false, \ \true, \
        \false, \ \false, \ \false, \ \true, \ \true, \ \false, \false, \
        \text{Four}, \ \false, \ \false, \false\right],
        \\
        {} & \mu^{(5)} = \left[\false, \ \false, \ \true, \ \false, \ \true, \
        \false, \ \true, \ \true, \ \true, \ \true, \ \false, \ \false, \
        \text{Four}, \ \false, \false, \false\right],
        \\
        {} & \mu^{(6)} = \left[\false, \ \false, \ \false, \ \true, \ \false, \
        \false, \ \true, \ \false, \ \true, \ \true, \ \false, \ \false, \
        \text{Four}, \ \true, \ \false, \ \false\right],
        \\
        {} & \mu^{(7)} = \left[\true, \ \false, \ \true, \ \false, \ \false, \
        \false, \ \true, \ \true, \ \true, \ \true, \ \false, \ \false, \ 
        \text{Four}, \ \true, \ \false, \ \true\right]\}\\
    \end{aligned}
    \end{equation}

    Finally, we take each element of \(\bar{\mu}\) in turn and replace it with
    the most similar point in our dataset such that no point is used twice.
    These \(k\) modes are given in Table~\ref{tab:huang-modes}.
    We also stipulate that if there is more than one entry in the set with the 
    same attributes as another (for instance, aardvark and bear are identical in
    this setting) that only one of these entries may be used as a replacement 
    for one of our virtual modes.\\

    \begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \input{tex/huang-modes.tex}
    }
    \caption{The \(7\) initial modes found by Huang's
    method.}\label{tab:huang-modes}
    \end{table}
\end{example}

\subsection{Cao's method}\label{subsec:cao}

Cao's method selects representative points by the average density of a point in
the dataset. As will be seen in the following definition, this average density 
is in fact the average relative frequency of all the attribute values of that 
point. This method is considered deterministic as there is no probabilistic 
element - unlike the standard or Huang's method - and so results are completely
reproducible.\\

\begin{definition}\label{def:density}	
    Consider a data set \(\textbf{X}\) with attribute set \(\textbf{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average density} of any point 
    \(X_i \in \textbf{X}\) with respect to \(\textbf{A}\) is 
    defined~\cite{Cao09} as:
	\[
	    \text{Dens}(X^{(i)}) = \frac{\sum_{j=1}^m \text{Dens}_{j}(X^{(i)})}{m}, 
        \quad \text{where} \quad \text{Dens}_{j}(X^{(i)}) = \frac{|\{X^{(t)} \in 
        \textbf{X} : x_j^{(i)} = x_j^{(t)}\}|}{N} = \frac{n(x_j^{(i)})}{N}
	\]\\
\end{definition}

\begin{remark}
    It is worth noting that we have \(\frac{1}{N} \leq \text{Dens}(X^{(i)})
    \leq 1\), since for any \(A_j \in \textbf{A}\):		
	\begin{itemize}	
        \item If \(n(x_j^{(i)}) = 1\), then \(\text{Dens}(X^{(i)}) = 
			\frac{\sum_{j=1}^m \frac{1}{N}}{m} = \frac{m}{mN} = \frac{1}{N}\)
        \item If \(n(x_j^{(i)}) = N\), then \(\text{Dens}(X^{(i)}) = 
            \frac{\sum_{j=1}^m 1}{m} = \frac{m}{m} = 1\)\\
	\end{itemize}
\end{remark}

\noindent Observe that:
\[
	|\{X^{(t)} \in \textbf{X} : x_j^{(i)} = x_j^{(t)}\}| = n(x_j^{(i)}) = 
	\sum_{t=1}^N (1 - \delta(x_j^{(i)}, x_j^{(t)}))
\]\\

and so, we can find an alternative definition for \(\text{Dens}(X^{(i)})\):
\begin{equation}
\begin{aligned}
    \text{Dens}(X^{(i)}) = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         (1 - \delta(x_j^{(i)}, x_j^{(t)}))
    \\
			             = {} & {} \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1 - 
                         \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 
                         \delta(x_j^{(i)}, x_j^{(t)})
    \\
                         = {} & {} \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N 
                         d(X^{(i)}, X^{(t)})
    \\
			             = {} & {} 1 - \frac{1}{mN} D(\textbf{X}, X^{(i)})
\end{aligned}
\end{equation}\\

With this alternative definition, we see - since \(m\) and \(N\) are fixed 
positive integers - that \(\text{Dens}(X^{(i)})\) is maximised when 
\(D(\textbf{X}, X^{(i)})\) is minimised. Then by Theorem \ref{thm:1} we have
that such an \(X^{(i)}\) with maximal average density in \textbf{X} with respect
to \textbf{A} is, in fact, a mode of \textbf{X}. This notion helps justify the 
method proposed by Cao et al. as discussed below.\\

\begin{algorithm}[H]
\caption{Cao's method}\label{alg:cao}
	\begin{algorithmic}[0]
        \State{\(\bar{\mu} \gets \emptyset\)}
        \For{\(X^{(i)} \in \textbf{X}\)}
            \State{Calculate \(\text{Dens}(X^{(i)})\).}
		\EndFor
        \State{Select \(X^{(i_1)} \in \textbf{X}\) which satisfies:
        \[
            \text{Dens}(X^{(i_1)}) = \max_{X^{(i)} \in \textbf{X}} 
            \{\text{Dens}(X^{(i)})\}
        \]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_1)}\)}
        \State{Select \(X^{(i_2)} \in \textbf{X}\) such that: 
		\[
			\text{Dens}(X^{(i_2)}) \times d(X^{(i_1)}, X^{(i_2)}) =
			\max_{X^{(i)} \in \textbf{X}} \{d(X^{(i)}, X^{(i_1)})\}
		\]
        }
        \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_2)}\)}
        \While{\(|\bar{\mu}| < k\)}
            \State{Select \(X^{(i_t)} \in \textbf{X}\) such that for all 
            \(\mu^{(l)} \in \bar{\mu}\):
			\[
		        d(X^{(i_t)}, \mu^{(l)}) \times \text{Dens}(X_{i_t}) = 
                \max_{X^{(i)} \in \textbf{X}} \{\min_{\mu^{(l)} \in \bar{\mu}} 
				\{d(X^{i}, \mu^{(l)}) \times \text{Dens}(X^{(i)}) \}\}
			\]
            }
            \State{\(\bar{\mu} \gets \bar{\mu} \cup X^{(i_t)}\)}
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\begin{example}\label{ex:cao}
    Cao's method with zoo animal dataset
\end{example}

