The \(k\)-modes algorithm is a part of the family of clustering algorithms known 
as `prototype-based clustering', and is an extension of the \(k\)-means 
algorithm for categorical data as set out in~\cite{Huang98}. This work will 
outline the key differences between the two algorithms, and then aim to 
examine how the initial cluster selection process has an impact on the 
efficiency and quality of the \(k\)-modes algorithm.\\


\subsection{Notation}\label{subsec:notation}

We will use the following notation throughout this work to describe our dataset,
its points, our clusters and their representative points:

\begin{itemize}
    \item Our dataset has \(N\) elements and is denoted by \textbf{X}.
    \item \textbf{X} is described by a set of \(m \in \mathbb{Z}_+\) attributes 
        \(\textbf{A} = \{A_1, \ldots, A_m\}\).
    \item Each attribute \(A_j\) draws its values from a set \(dom(A_j) = 
        \{a_1^{(j)}, \ldots, a_{d_j}^{(j)}\}\) where \(d_j = |dom(A_j)| \in 
        \mathbb{Z}_+\) is sometimes used as shorthand.
    \item We write each data point \(X^{(i)}\) as an \(m\)-dimensional vector:
	\[
		X^{(i)} = \left[A_1 = x_1^{(i)}, A_2 = x_2^{(i)}, \ldots, A_m = 
        x_m^{(i)}\right], \ \ i=1, \ldots, N
	\]
        where \(x_j^{(i)}\) is the value of the \(j^{th}\) attribute of the
        \(i^{th}\) data point, \(X^{(i)}\).
	\item Prototype-based clustering algorithms partition the elements of 
        \(\textbf{X}\) into \(k\) distinct sets (clusters) denoted by \(C_1, 
        \ldots, C_k\), where \(k \in \mathbb{Z}_+\) is a pre-determined, fixed 
        integer such that \(k \le N\). That is:
	\[
		C_1, \ldots, C_k \text{ are such that } \bigcup_{l=1}^k C_l = 
		\textbf{X} \quad \text{and} \quad C_l \cap C_t = \emptyset 
		\text{ for all } l \neq t
	\]
    \item Each cluster \(C_l\) has associated with it a representative point 
		(see: Section~\ref{subsec:rep-points}) which we denote by 
        \(\mu^{(l)}~=~\left[\mu_1^{(l)},~\ldots,~\mu_m^{(l)}\right]\).
\end{itemize}


\subsection{Dissimilarity measure}\label{subsec:dissim}

An immediate difference between the \(k\)-means and \(k\)-modes algorithms is 
that they deal with different types of data, and so the metric used to define 
the distance between two points in our space must be different. With 
\(k\)-means, where the data has all-numeric attributes, Euclidean distance is 
often used. However, we do not have this sense of distance with categorical 
data. Instead, we utilise a dissimilarity measure - defined below - as our 
metric. It can be easily checked that this is indeed a distance measure.\\

\begin{definition}\label{def:dissim}
	Let $\textbf{X}$ be a data set and consider $X^{(a)}, X^{(b)} \in 
    \textbf{X}$. We define the \emph{dissimilarity} between $X^{(a)}$ and 
    $X^{(b)}$, denoted by \(d(X^{(a)}, X^{(b)})\), to be:
	\[
	d(X^{(a)}, X^{(b)}) = \sum_{j=1}^{m} \delta(x_j^{(a)}, x_j^{(b)}) \quad
	\text{where} \quad \delta(x, y) = \begin{cases}
                                        0, & x = y \\
					                    1, & \text{otherwise}
					                  \end{cases}
	\]

    In other words, the dissimilarity between two points is simply the number of
    attributes where their values are not the same. It should also be clear that
    the dissimilarity between a point and itself is always zero.
\end{definition}


\begin{example}\label{ex:dissim}
    Throughout this paper, we will make use of a small numerical example to aid 
    our understanding of concepts. The dataset we will use is available online 
    at~\url{https://www.kaggle.com/uciml/zoo-animal-classification/data}.\\

    This dataset is built for the classification of zoo animals. There are 101
    instances, described by 16 (mostly binary) attributes, their name, and a 
    class label indicating the type of animal they are. The first five rows of 
    the dataset are given in Table~\ref{tab:zoo-head}. Please note that the
    first column is the index starting at \(1\).\\
    
    \begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \centering
        \input{tex/zoo-head.tex}
    }
    \caption{The head of the zoo animal dataset}\label{tab:zoo-head}
    \end{table}

    Let us consider three of our points: aardvark, antelope and bass. Then our
    attributes are the central 16 columns (from `hair' to `catsize'), and with 
    our notation, these points can be expressed as the following vectors:

    \begin{equation}
    \nonumber
    \begin{aligned}
        \textbf{Aardvark:} \quad & X^{(0)} = \left[x_1^{(0)} = \true, \ 
            x_2^{(2)} = \false, \ldots, \ x_{15}^{(0)} = \false, \ x_{16}^{(0)} 
            = \true\right]
        \\
        \textbf{Antelope:} \quad & X^{(1)} = \left[x_1^{(1)} = \true, \
            x_2^{(1)} = \false, \ldots, \ x_{15}^{(1)} = \false, \ x_{16}^{(1)} 
            = \true\right]
        \\
        \textbf{Bass:} \quad & X^{(2)} = \left[x_1^{(2)} = \false, \ x_2^{(2)} = 
            \false, \ \ldots, \ x_{15}^{(2)} = \false, \ x_{16}^{(2)} = 
            \false\right]
        \\
    \end{aligned}
    \end{equation}

    And using Definition~\ref{def:dissim}, their pairwise dissimilarities are:
    \begin{equation}
    \nonumber
    \begin{aligned}
        d(X^{(0)}, X^{(1)}) & = & 0 + 0 + 0 + 0 + 0 + 0 + 1 + 0 + 0 + 0 + 0 + 0 
        + 0 + 1 + 0 + 0 & = 2
        \\
        d(X^{(0)}, X^{(2)}) & = & 1 + 0 + 1 + 1 + 0 + 1 + 0 + 0 + 0 + 1 + 0 + 1 
        + 1 + 1 + 0 + 1 & = 9
        \\
        d(X^{(1)}, X^{(2)}) & = & 1 + 0 + 1 + 1 + 0 + 1 + 1 + 0 + 0 + 1 + 0 + 1
        + 1 + 0 + 0 + 1 & = 9
        \\
    \end{aligned}
    \end{equation}
    \\
\end{example}

\subsection{Representative points}\label{subsec:rep-points}

Now that we have defined a metric on our space, we can turn our attention to 
what we mean by the representative point \(\mu^{(l)}\) of a cluster \(C_l\). In 
\(k\)-means, we call \(\mu^{(l)}\) a `centroid' and define it to be the average 
of all points \(X^{(i)} \in C_l\) by Euclidean distance. With categorical data, 
we use our revised distance measure from Definition~\ref{def:dissim} to specify 
a representative point. We call such a point a mode of \textbf{X}.\\

\begin{definition}\label{def:mode}
    We define a \emph{mode} of our set \textbf{X} to be any vector \(\mu = 
    [\mu_1, \ldots, \mu_m]\) that minimises the \emph{summed dissimilarity}:
	
    \begin{equation}
        D(\textbf{X}, \mu) = \sum_{i=1}^{N} d(X^{(i)}, \mu)
	\end{equation}
	
    Note that \(\mu\) is not necessarily in \textbf{X}. We call such a vector
    \(\mu\) a \emph{virtual mode} of \textbf{X}.
\end{definition}

\begin{definition}\label{def:rel-freq}
    Let \textbf{X} be a dataset with attributes \(A_1, \ldots, A_m\). Then we
    denote by \(n(a_s^{(j)})\) the \emph{frequency} of the \(s^{th}\) category 
    \(a_s^{(j)}\) of \(A_j\) in \textbf{X}. That is, 
	
    \[
	    n(a_s^{(j)}) := |{\{X^{(i)} \in \textbf{X}: x_j^{(i)} = a_s^{(j)}\}}|
	\]
	
    We call \(\frac{n(a_s^{(j)})}{N}\) the \emph{relative frequency} of category 
    \(a_s^{(j)}\) in \textbf{X}.
\end{definition}

\begin{remark}
    Note that we have \(1 \le n(a_s^{(j)}) \le N\) for all \(s\) and \(j = 1, 
    \ldots, m\).\\
\end{remark}

\begin{theorem}\label{thm:1}
    Consider a dataset \textbf{X} and some \(X^{(i)} \in \textbf{X}\). Then:
	
    \[
	    D(\textbf{X}, X^{(i)}) \text{ is minimised } \iff n(x_j^{(i)}) \geq 
	    n(a_s^{(j)}) \text{ for all } s \text{ and } j = 1, \ldots, m 
	\]
\end{theorem}
A proof of this theorem can be found in the Appendix of~\cite{Huang98}.\\

\begin{example}\label{ex:mode}
    Let us return to our zoo animal dataset from Example~\ref{ex:dissim}. Using 
    Theorem~\ref{thm:1}, we can identify a mode of our set by taking the most 
    commonly occurring value for each attribute. This gives us the following 
    vector:
    \[
        \mu = \left[\false, \ \false, \ \true, \ \false, \ \false, \ \false, \
        \true, \ \true, \ \true, \ \true, \ \false, \false, \ \text{Four}, \ 
        \true, \ \false, \ \false\right]
    \]

    In fact, this vector appears in our dataset, and corresponds to the entry 
    for the tuatara:
    
    \begin{table}[h]
        \resizebox{\textwidth}{!}{%
        \centering
        \input{tex/tuatara.tex}\label{tab:tuatara}
    }
    \end{table}

    To be consistent with our definition for a mode, we should also calculate 
    our summed dissimilarity:
    \[
        D(\textbf{X}, \mu) = 5 + 5 + 4 + 5 + \cdots + 6 + 8 + 4 + 5 + 5 = 501
    \]
\end{example}

\subsection{The cost function}\label{subsec:cost}

We can use Definitions~\ref{def:dissim}~\&~\ref{def:mode} to determine a cost 
function for our algorithm. Let \(\bar{\mu} = \{\mu^{(1)}, \ldots, \mu^{(k)}\}\) 
be a set of \(k\) modes of \textbf{X}, and let \(W = (w_{i,l})\) be an \(N 
\times k\) matrix such that:

\[ 
    w_{i,l} = \begin{cases}
                1, & X^{(i)} \in C_l \\
                0, & \text{otherwise}
              \end{cases}
\]\\

Then we define our \emph{cost function} to be the summed within-cluster 
dissimilarity:

\begin{equation}
    \text{Cost}(W, \bar{\mu}) = \sum_{l=1}^{k} \sum_{i=1}^{N} 
                                \sum_{j=1}^{m} w_{i,l} 
                                \delta(x_{i,j}, \mu_{l,j})
\end{equation}


\subsection{The \(k\)-modes algorithm}\label{subsec:kmodes}

Below is a practical implementation of the \(k\)-modes algorithm~\cite{Huang98}:

\begin{algorithm}[H]
    \caption{\(k\)-modes}\label{alg:kmodes}
	\begin{algorithmic}[0] 
        \State{\(\bar{\mu} \gets \emptyset\)}
        \For{\(l \in \{1, \ldots, k\}\)}
            \State{\(C_l \gets \emptyset\)}
		\EndFor
        \State{Select \(k\) initial modes, \(\mu^{(1)}, \ldots, \mu^{(k)}\).}
        \State{\(\bar{\mu} \gets \{\mu^{(1)}, \ldots, \mu^{(k)}\}\)}
        \For{\(X_i \in \textbf{X}\)}
            \State{Select \(l^* \text{ that satisfies } \displaystyle{d(X^{(i)}, 
                \mu^{(l^*)}) = \min_{1 \le l \le m} \{d(X^{(i)},
                \mu^{(l)})\}}\).}
            \State{\(C_{j^*} \gets C_{j^*} \cup \{X^{(i)}\}\)}
            \State{Update \(\mu^{(l^*)}\).}
		\EndFor
        \Repeat{%
            \For{\(X^{(i)} \in \textbf{X}\)}
                \For{\(\mu^{(l)} \in \bar{\mu}\)}
                    \State{Calculate \(d(X^{(i)}, \mu^{(l)})\)}
				\EndFor
                \If{\(d(X^{(i)}, \mu^{(l^*)}) > d(X^{(i)}, \mu^{(l')}) \text{ 
                for some } l' \neq l^*\)}
                    \State{\(C_{l^*} \gets C_{l^*} \setminus \{X^{(i)}\}\)}
                    \State{\(C_{l'} \gets C_{l'} \cup \{X^{(i)}\}\)}
                    \State{Update both \(\mu^{(l^*)} \text{ and } \mu^{(j')}\).}
				\EndIf
			\EndFor
        }
		\Until{No point changes cluster after a full cycle through \textbf{X}.}
	\end{algorithmic}
\end{algorithm}

\begin{remark}
    The processes by which the \(k\) initial modes are selected are detailed in 
    Sections~\ref{sec:init}~\&~\ref{sec:proposed-method}.
\end{remark}

