\section{Introduction}\label{sec:intro}

This work focusses on \(k\)-modes clustering --- an extension to
\(k\)-means that permits the sensible clustering of categorical (i.e.\ ordinal,
nominal or otherwise discrete) data as set out in the seminal works by
Huang~\cite{Huang1997a,Huang1997b,Huang1998}. In particular, the interest of
this paper is in how the performance of the \(k\)-modes algorithm is affected by
the quality of its initial solution. The initialisation method proposed in this
work extends the method presented by Huang~\cite{Huang1998} by using results
from game theory to ensure mathematical fairness and to lever the full learning
opportunities presented by the data being clustered. In doing so, it is
demonstrated that the proposed method is able to outperform both of the
established initialisations for \(k\)-modes. The paper is structured as follows:
\begin{itemize}
    \item Section~\ref{sec:intro} introduces the \(k\)-modes algorithm and its
        established initialisation methods.
    \item Section~\ref{sec:method} provides a brief overview of
        matching games and their variants before a statement of the proposed
        initialisation.
    \item Section~\ref{sec:results} presents analyses of the initialisations
        on benchmark and new, artificial datasets.
    \item Section~\ref{sec:conclusion} concludes the paper.
\end{itemize}


\subsection{The \(k\)-modes algorithm}\label{subsec:kmodes}

The following notation will be used throughout this work to describe the objects
associated with clustering a categorical dataset:

\begin{itemize}
    \item Let \(\mathcal{A} := A_1 \times \cdots \times A_m\) denote the
        \emph{attribute~space}. In this work, only categorical attributes are
        considered, i.e.\ for each \(j = 1, \ldots, m\) it follows that \(A_j :=
        \left\{a_1^{(j)}, \ldots, a_{d_j}^{(j)}\right\}\) where \(d_j = |A_j|\)
        is the size of the \(j^{th}\) attribute.

    \item Let \(\mathcal{X} := \left\{X^{(1)}, \ldots, X^{(N)}\right\} \subset
        \mathcal{A}\) denote a \emph{dataset} where each \(X^{(i)} \in
        \mathcal{X}\) is defined as an \(m\)-tuple \(X^{(i)} := \left(x_1^{(i)},
        \ldots, x_m^{(i)}\right)\) where \(x_j^{(i)} \in A_j\) for each \(j = 1,
        \ldots, m\). The elements of \(\mathcal{X}\) are referred to as
        \emph{data points} or \emph{instances}.
    \item Let \(\mathcal{Z} := \left(Z_1, \ldots, Z_k\right)\) be a partition
        of a dataset \(\mathcal{X} \subset \mathcal A\) into \(k \in
        \mathbb{Z}^{+}\) distinct, non-empty parts. Such a partition
        \(\mathcal{Z}\) is called a \emph{clustering} of \(\mathcal{X}\).

    \item Each cluster \(Z_l\) has associated with it a
        \emph{mode} (see Definition~\ref{def:mode}) which is
        denoted by \(z^{(l)} = \left(z_1^{(l)},~\ldots,~z_m^{(l)}\right) \in
        \mathcal{A}\).  These points are also referred to as
        \emph{representative~points} or \emph{centroids}. The set of all current
        cluster modes is denoted as \(\overline Z = \left\{z^{(1)}, \ldots,
        z^{(k)}\right\}\).
\end{itemize}

Definition~\ref{def:dissim} describes a dissimilarity measure between
categorical data points.

\begin{definition}\label{def:dissim}
    Let \(\mathcal{X} \subset \mathcal A\) be a dataset and consider any
    \(X^{(a)}, X^{(b)} \in \mathcal{X}\). The dissimilarity between \(X^{(a)}\)
    and \(X^{(b)}\), denoted by \(d\left(X^{(a)}, X^{(b)}\right)\), is given by:
    \begin{equation}\label{eq:dissim}
        d\left(X^{(a)}, X^{(b)}\right) := \sum_{j=1}^{m} \delta\left(x_j^{(a)},
        x_j^{(b)}\right) \quad \text{where} \quad \delta\left(x, y\right) =
        \begin{cases}
            0, & \text{if} \ x = y \\
            1, & \text{otherwise.}
        \end{cases}
    \end{equation}
\end{definition}

With this metric, the notion of a representative point of a cluster is
addressed. With numeric data and \(k\)-means, such a point is taken to be the
mean of the points within the cluster. With categorical data, however, the mode
is used as the measure for central tendency. This follows from the concept of
dissimilarity in that the point that best represents (i.e.\ is closest to) those
in a cluster is one with the most frequent attribute values of the points in the
cluster. The following definitions and theorem formalise this and a method to
find such a point.

\begin{definition}\label{def:mode}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset and consider some point
    \(z = \left(z_1, \ldots, z_m\right) \in \mathcal{A}\). Then \(z\) is called
    a \emph{mode} of \(\mathcal{X}\) if it minimises the following:
    \begin{equation}\label{eq:summed-dissim}
        D\left(\mathcal{X}, z\right) = \sum_{i=1}^{N} d\left(X^{(i)}, z\right)
    \end{equation}
\end{definition}

\begin{definition}\label{def:rel-freq}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset. Then
    \(n\left(a_s^{(j)}\right)\) denotes the \emph{frequency} of the \(s^{th}\)
    category \(a_s^{(j)}\) of \(A_j\) in \(\mathcal{X}\), i.e.\ for each \(A_j
    \in \mathcal{A}\) and each \(s = 1, \ldots, d_j\):
    \begin{equation}
        n\left(a_s^{(j)}\right) := \abs*{%
            {\left\{X^{(i)} \in \mathcal{X}: x_j^{(i)} = a_s^{(j)}\right\}}
        }
    \end{equation}
	
    Furthermore, \(\frac{n\left(a_s^{(j)}\right)}{N}\) is called the
    \emph{relative~frequency} of category \(a_s^{(j)}\) in \(\mathcal{X}\).
\end{definition}

\begin{theorem}\label{thm:mode}
    Consider a dataset \(\mathcal{X} \subset \mathcal{A}\) and some \(U = (u_1,
    \ldots, u_m) \in \mathcal{A}\). Then \(D(\mathcal{X}, U)\) is minimised if
    and only if \(n\left(u_j\right) \geq n\left(a_s^{(j)}\right)\) for all
    \(s=1, \ldots, d_j\) for each \(j = 1, \ldots, m\).

    A proof of this theorem can be found in the Appendix of~\cite{Huang1998}.
\end{theorem}

Theorem~\ref{thm:mode} defines the process by which cluster modes are updated
in \(k\)-modes (see Algorithm~\ref{alg:update}), and so the final component from
the \(k\)-means paradigm to be configured is the objective (cost) function. This
function is defined in Definition~\ref{def:cost}, and following that a practical
statement of the \(k\)-modes algorithm is given in Algorithm~\ref{alg:kmodes} as
set out in~\cite{Huang1998}.

\begin{definition}\label{def:cost}
    Let \(\mathcal{Z} = \left\{Z_1, \ldots, Z_k\right\}\) be a clustering of a
    dataset \(\mathcal{X}\), and let \(\overline Z = \left\{z^{(1)},
    \ldots, z^{(k)}\right\}\) be the corresponding cluster modes. Then \(W =
    \left(w_{i, l}\right)\) is an \(N \times k\) \emph{partition~matrix} of
    \(\mathcal{X}\) such that:
    \[
        w_{i, l} = \begin{cases}
                     1, & \text{if} \ X^{(i)} \in Z_l\\
                     0, & \text{otherwise.}
                   \end{cases}
    \]

    With this, the \emph{cost~function} is defined to be the summed
    within-cluster dissimilarity:
    \begin{equation}\label{eq:cost}
        C\left(W, \overline Z\right) := \sum_{l=1}^{k} \sum_{i=1}^{N}
        \sum_{j=1}^{m} w_{i,l} \ \delta\left(x_j^{(i)}, z_j^{(l)}\right)
    \end{equation}
\end{definition}

\input{tex/algorithms/kmodes.tex}


\subsection{Initialisation processes}\label{subsec:inits}

The standard selection method to initialise \(k\)-modes is to randomly sample
\(k\) distinct points in the dataset. In all cases, the initial modes must be
points in the dataset to ensure that there are no empty clusters in the first
iteration of the algorithm. The remainder of this section describes two
well-established initialisation methods that aim to preemptively lever the
structure of the data at hand.

\subsubsection{Huang's method}\label{subsec:huang}

Amongst the original works by Huang, an alternative initialisation method was
presented that selects modes by distributing frequently occurring values from
the attribute space among \(k\) potential modes~\cite{Huang1998}. The process,
denoted as Huang's method, is described in full in Algorithm~\ref{alg:huang}.
Huang's method considers a set of potential modes, \(\widehat Z \subset \mathcal
A\), that is then replaced by the actual set of initial modes, \(\overline Z
\subset \mathcal X\). The statement of how the set of potential modes are formed
is ambiguous in the original paper --- as is alluded to in~\cite{Jiang2016}.
Here, as is done in practical implementations of \(k\)-modes, this has been
interpreted as being done via a weighted random sample (see
Algorithm~\ref{alg:potential_modes}).

\input{tex/algorithms/huang.tex}


\subsubsection{Cao's method}\label{subsec:cao}

The second initialisation process that is widely used with \(k\)-modes is known
as Cao's method~\cite{Cao2009}. This method selects the initial modes according
to their density in the dataset whilst forcing dissimilarity between them.
Definition~\ref{def:density} formalises the concept of density and its
relationship to relative frequency. The method, which is described in
Algorithm~\ref{alg:cao}, is deterministic --- unlike Huang's method which relies
on random sampling.

\begin{definition}\label{def:density}	
    Consider a dataset
    \(\mathcal{X} \subset \mathcal{A} = \{A_1, \ldots, A_m\}\). Then the
    \emph{average~density} of any point \(X_i \in \mathcal{X}\) with respect to
    \(\mathcal{A}\) is defined~\cite{Cao2009} as:
    \begin{equation}\label{eq:density}
        \text{Dens}\left(X^{(i)}\right) = \frac{%
            \sum_{j=1}^m \text{Dens}_{j}\left(X^{(i)}\right)
        }{m}
        \quad \text{where} \quad
        \text{Dens}_{j}\left(X^{(i)}\right) = \frac{%
            \abs*{%
                \left\{X^{(t)} \in \mathcal{X} : x_j^{(i)} = x_j^{(t)}\right\}
            }
        }{N}
    \end{equation}

    Observe that:
    \[
        \abs*{\left\{X^{(t)} \in \mathcal{X} : x_j^{(i)} = x_j^{(t)}\right\}}%
        = n\left(x_j^{(i)}\right)%
        = \sum_{t=1}^N \left(1 - \delta\left(x_j^{(i)}, x_j^{(t)}\right)\right)
    \]

    And so, an alternative definition for~\eqref{eq:density} can be derived:
    \begin{equation}\label{eq:density-alt}
        \text{Dens}\left(X^{(i)}\right)
        = \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N \left(%
            1 - \delta\left(x_j^{(i)}, x_j^{(t)}\right)
        \right)
        = 1 - \frac{1}{mN} D\left(\mathcal{X}, X^{(i)}\right)
    \end{equation}
\end{definition}

\input{tex/algorithms/cao.tex}
