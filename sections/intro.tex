\section{Introduction}\label{sec:intro}

Clustering is an unsupervised learning technique for discovering intrinsic
structure within data. There exist many approaches to clustering but perhaps the
most ubiquitous amongst them is centroid-based clustering. This approach aims to
maximise summed within-cluster similarity by iterating over the points in a
dataset and adjusting the current clusters according to some measure for central
tendency until convergence. A popular algorithm for performing centroid-based
clustering is the \(k\)-means algorithm. In \(k\)-means, a number of groups to
identify in a dataset, \(k\), is fixed \emph{a priori} and each cluster has
associated with it a centroid or representative point calculated as the mean of
the data points within that cluster. Unfortunately, this is only valid for
numeric data where the mean of a set is well-defined. Despite this, the paradigm
in which \(k\)-means clustering exists is of interest as it is fast, scalable,
easily parallelised, and simple in its design~\cite{Wu2009,Zhao2009}.

In this work, the focus will be on \(k\)-modes clustering; an extension to
\(k\)-means that permits the sensible clustering of categorical (i.e.\ ordinal,
nominal or otherwise discrete) data as set out in the seminal works by
Huang~\cite{Huang1997a,Huang1997b,Huang1998}. An alternative, though largely
equivalent, form for \(k\)-modes was presented in~\cite{Chaturvedi2001} but is
not considered in this work. Under this framework, the central tendency measure
used is the mode and Euclidean distance is replaced by a simple matching
measure. All of the concepts used to define the \(k\)-modes algorithm are
presented later in this section.

The interest of this paper is in how the performance of the \(k\)-modes
algorithm may be affected. Since the \(k\)-modes algorithm is a heuristic, its
performance is dependent on its initial solution. The quality of the initial
solution is affected by two components: the metric being used and the process by
which the solution is chosen.

Strictly, introducing a new metric alters the space in which the data exists and
its effect on the initial solution is not independent of the final solution.
Having said that, following the seminal \(k\)-modes papers, a number of
alternative dissimilarity measures have been implemented to improve on the
simple matching dissimilarity used most regularly. The main drawback of the
standard measure is that it often produces clusters with low intra-cluster
similarity~\cite{Ng2007} and does not take into account any relationships
between attributes or their categories. Other measures have been designed to be
used in a specific context where such relationships may be
considered~\cite{Cao2012,Yu2018,Zhou2016}. However, these measures sometimes are
defined between a point in the dataset and a centroid rather than defining a
metric for the entire space.

Instead of adjusting the overall space, this work considers the process by which
an initial solution is found. The proposed method is an extension to that
presented by Huang~\cite{Huang1998} that generates a game-theoretically fair and
stable variant to that generated by Huang's method. The remainder of this paper
is structured as follows:
\begin{itemize}
    \item Section~\ref{sec:intro} introduces the \(k\)-modes algorithm and its
        established initialisation methods.
    \item Section~\ref{sec:method} provides a brief overview of
        matching games and their variants before a statement of the proposed
        initialisation method.
    \item Section~\ref{sec:results} presents analyses of the initialisation
        methods on benchmark and new, artificial datasets.
    \item Section~\ref{sec:conclusion} concludes the paper.
\end{itemize}


\subsection{The \(k\)-modes algorithm}\label{subsec:kmodes}

The following notation will be used throughout this work to describe the objects
associated with clustering a dataset:

\begin{itemize}
    \item Let \(\mathcal{A} := A_1 \times \cdots \times A_m\) denote the
        \emph{attribute~space}. In this work, only categorical attributes are
        considered and so it is intuitive to describe each attribute as a set of
        its values, i.e.\ for each \(j = 1, \ldots, m\) it follows that \(A_j :=
        \left\{a_1^{(j)}, \ldots, a_{d_j}^{(j)}\right\}\) where \(d_j = |A_j|\)
        is considered the size of the \(j^{th}\) attribute.

    \item Let \(\mathcal{X} := \left\{X^{(1)}, \ldots, X^{(N)}\right\} \subset
        \mathcal{A}\) denote a \emph{dataset} where each \(X^{(i)} \in
        \mathcal{X}\) is defined as an \(m\)-tuple \(X^{(i)} := \left(x_1^{(i)},
        \ldots, x_m^{(i)}\right)\) where \(x_j^{(i)} \in A_j\) for each \(j = 1,
        \ldots, m\). The elements of \(\mathcal{X}\) are referred to as
        \emph{data points} or \emph{instances}.
%        A dataset \(\mathcal{X}\) can be
%        represented as a table like so:
%        \begin{table}[H]
%        \centering
%        \begin{tabular}{cccccc}
%            {} & \(A_1\) & \(A_2\) & \quad \ldots \quad & \(A_{m-1}\) & \(A_m\)
%            \\
%            \midrule
%            \(X^{(1)}\) & \(x_1^{(1)}\) & \(x_2^{(1)}\) & \quad \ldots \quad & 
%            \(x_{m-1}^{(1)}\) & \(x_m^{(1)}\)
%            \\
%            \(X^{(2)}\) & \(x_1^{(2)}\) & \(x_2^{(2)}\) & \quad \ldots \quad &
%            \(x_{m-1}^{(2)}\) & \(x_m^{(2)}\)
%            \\
%            \vdots & \vdots & \vdots & {} & \vdots & \vdots
%            \\
%            \(X^{(N)}\) & \(x_1^{(N)}\) & \(x_2^{(N)}\) & \quad \ldots \quad &
%            \(x_{m-1}^{(N)}\) & \(x_m^{(N)}\)
%        \end{tabular}
%        \end{table}

    \item Let \(\mathcal{Z} := \left(Z_1, \ldots, Z_k\right)\) be a partition
        of a dataset \(\mathcal{X}\) into \(k \in \mathbb{Z}^{+}\) distinct,
        non-empty parts. Such a partition \(\mathcal{Z}\) is called a
        \emph{clustering} of \(\mathcal{X}\).

    \item Each cluster \(Z_l\) has associated with it a
        \emph{representative~point} (see Definition~\ref{def:mode}) which is
        denoted by \(z^{(l)} = \left(z_1^{(l)},~\ldots,~z_m^{(l)}\right) \in
        \mathcal{A}\).  These points may also be referred to as cluster modes.
        The set of all current representative points is denoted \(\overline Z =
        \left\{z^{(1)}, \ldots, z^{(k)}\right\}\).
\end{itemize}

As is discussed above, the notion of distance is lost in categorical space, and
especially when that space is even partly nominal. Definition~\ref{def:dissim}
describes a simple dissimilarity measure between categorical data points.

\begin{definition}\label{def:dissim}
    Let \(\mathcal{X}\) be a dataset and consider any \(X^{(a)}, X^{(b)} \in
    \mathcal{X}\). The dissimilarity between \(X^{(a)}\) and \(X^{(b)}\),
    denoted by \(d\left(X^{(a)}, X^{(b)}\right)\), is given by:
    \begin{equation}\label{eq:dissim}
        d\left(X^{(a)}, X^{(b)}\right) := \sum_{j=1}^{m} \delta\left(x_j^{(a)},
        x_j^{(b)}\right) \quad \text{where} \quad \delta\left(x, y\right) =
        \begin{cases}
            0, & \text{if} \ x = y \\
            1, & \text{otherwise.}
        \end{cases}
    \end{equation}
    In other words, the dissimilarity between two points is the number of
    attributes where their values are not the same. A proof
    that~\eqref{eq:dissim} is a valid distance metric is given as an appendix.
\end{definition}

%\input{tex/examples/dissim.tex}

With this metric defined, the notion of a representative point within a cluster
can be addressed. When clustering numeric data, a centroid of a cluster is taken
to be the average of the points within the cluster so as to summarise the
information contained within that cluster. With categorical data, however, a
frequency approach is used. This follows from the concept of dissimilarity
where the point that best represents (i.e.\ is closest to) those in a cluster
is one with the most frequent attribute values of the points in the cluster. As
such, a representative point of a cluster is often called a mode. The following
definitions and theorem formally define such a representative point and a means
of finding them.

\begin{definition}\label{def:mode}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset and consider some point
    \(z = \left(z_1, \ldots, z_m\right) \in \mathcal{A}\). Then \(z\) is called
    a \emph{mode} of \(\mathcal{X}\) if it minimises the following:
    \begin{equation}\label{eq:summed-dissim}
        D\left(\mathcal{X}, z\right) = \sum_{i=1}^{N} d\left(X^{(i)}, z\right)
    \end{equation}
\end{definition}

\begin{definition}\label{def:rel-freq}
    Let \(\mathcal{X} \subset \mathcal{A}\) be a dataset. Then
    \(n\left(a_s^{(j)}\right)\) denotes the \emph{frequency} of the \(s^{th}\)
    category \(a_s^{(j)}\) of \(A_j\) in \(\mathcal{X}\), i.e.\ for each \(A_j
    \in \mathcal{A}\) and each \(s = 1, \ldots, d_j\):
    \begin{equation}
        n\left(a_s^{(j)}\right) := \abs*{%
            {\left\{X^{(i)} \in \mathcal{X}: x_j^{(i)} = a_s^{(j)}\right\}}
        }
    \end{equation}
	
    Furthermore, \(\frac{n\left(a_s^{(j)}\right)}{N}\) is called the
    \emph{relative~frequency} of category \(a_s^{(j)}\) in \(\mathcal{X}\).
\end{definition}

\begin{theorem}\label{thm:mode}
    Consider a dataset \(\mathcal{X} \subset \mathcal{A}\) and some \(U = (u_1,
    \ldots, u_m) \in \mathcal{A}\). Then \(D(\mathcal{X}, U)\) is minimised if
    and only if \(n\left(u_j\right) \geq n\left(a_s^{(j)}\right)\) for all
    \(s=1, \ldots, d_j\) for each \(j = 1, \ldots, m\).

    A proof of this theorem can be found in the Appendix of~\cite{Huang1998}.
\end{theorem}

%\input{tex/examples/mode.tex}

Theorem~\ref{thm:mode} defines the process by which representatives are updated
in \(k\)-modes (see Algorithm~\ref{alg:update}), and so the final component from
the \(k\)-means paradigm to be configured is the objective (cost) function. This
function is defined in Definition~\ref{def:cost}, and following that a practical
statement of the \(k\)-modes algorithm is given in Algorithm~\ref{alg:kmodes} as
set out in~\cite{Huang1998}.

\begin{definition}\label{def:cost}
    Let \(\mathcal{Z} = \left\{Z_1, \ldots, Z_k\right\}\) be a clustering of a
    dataset \(\mathcal{X}\), and let \(\overline Z = \left\{z^{(1)},
    \ldots, z^{(k)}\right\}\) be the corresponding cluster modes. Then \(W =
    \left(w_{i, l}\right)\) is an \(N \times k\) \emph{partition~matrix} of
    \(\mathcal{X}\) such that:
    \[
        w_{i, l} = \begin{cases}
                     1, & \text{if} \ X^{(i)} \in Z_l\\
                     0, & \text{otherwise.}
                   \end{cases}
    \]

    With this, the \emph{cost~function} is defined to be the summed
    within-cluster dissimilarity:
    \begin{equation}\label{eq:cost}
        C\left(W, \overline Z\right) := \sum_{l=1}^{k} \sum_{i=1}^{N}
        \sum_{j=1}^{m} w_{i,l} \ \delta\left(x_j^{(i)}, z_j^{(l)}\right)
    \end{equation}
\end{definition}

\input{tex/algorithms/kmodes.tex}

The standard selection method to initialise \(k\)-modes is to randomly sample
\(k\) distinct points in the dataset. In all cases, the initial modes must be
points in the dataset to ensure that there are no empty clusters in the first
iteration of the algorithm. The remainder of this section describes two
well-established initialisation methods that aim to preemptively lever the
structure of the data at hand.


\subsection{Initialisation processes}\label{subsec:inits}

\subsubsection{Huang's method}\label{subsec:huang}

Amongst the original works by Huang, an alternative initialisation method was
presented that selects modes by distributing frequently occurring values from the
attribute space among \(k\) potential modes~\cite{Huang1998}. The process,
denoted as Huang's method, is described in full in Algorithm~\ref{alg:huang}.
Huang's method considers a set of potential modes,
\(\widehat Z \subset \mathcal A\), that is then replaced by the actual set of
initial modes, \(\overline Z \subset \mathcal X\).

In the original statement of Huang's method, it is stated that the most
frequent categories should be assigned `equally' to the set of potential modes.
How the categories should be distributed `equally' is not well-defined or easily
seen from the example given in the paper. In software implementations, including
the one used in Section~\ref{sec:results}, the term is taken to mean using a
probability distribution to sample values from the attribute space. This
probability distribution is formed by the relative frequencies of each
attribute's categories.

\input{tex/algorithms/huang.tex}
%\input{tex/examples/huang.tex}


\subsubsection{Cao's method}\label{subsec:cao}

The second initialisation process that is widely used with \(k\)-modes is known
as Cao's method~\cite{Cao2009}. This method selects representative points
according to their density in the dataset whilst forcing dissimilarity between
them. Definition~\ref{def:density} formalises the concept of density and its
relationship to relative frequency. The method, which is described in
Algorithm~\ref{alg:cao}, is often considered to be deterministic as there is no
formally stochastic element. However, this is only true up to an arbitrary
breaking of ties in the density-dissimilarity calculations and so many practical
implementations cannot guarantee a unique solution across multiple runs using
this method.

\begin{definition}\label{def:density}	
    Consider a data set \(\mathcal{X}\) with attribute set \(\mathcal{A} = 
    \{A_1, \ldots, A_m\}\). Then the \emph{average~density} of any point 
    \(X_i \in \mathcal{X}\) with respect to \(\mathcal{A}\) is 
    defined~\cite{Cao2009} as:
    \begin{equation}\label{eq:density}
        \text{Dens}\left(X^{(i)}\right) = \frac{%
            \sum_{j=1}^m \text{Dens}_{j}\left(X^{(i)}\right)
        }{m}
        \ \ \text{where} \ \
        \text{Dens}_{j}\left(X^{(i)}\right) = \frac{%
            \abs*{%
                \left\{X^{(t)} \in \mathcal{X} : x_j^{(i)} = x_j^{(t)}\right\}
            }
        }{N}
    \end{equation}

    Observe that:
    \[
        \abs*{\left\{X^{(t)} \in \mathcal{X} : x_j^{(i)} = x_j^{(t)}\right\}}%
        = n\left(x_j^{(i)}\right)%
        = \sum_{t=1}^N \left(1 - \delta\left(x_j^{(i)}, x_j^{(t)}\right)\right)
    \]

    And so, an alternative definition for~\eqref{eq:density} can be derived:
    \begin{equation}\label{eq:density-alt}
    \begin{aligned}
        \text{Dens}\left(X^{(i)}\right)
        & = \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N \left(%
            1 - \delta\left(x_j^{(i)}, x_j^{(t)}\right)
        \right)\\
        & = \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N 1%
            - \frac{1}{mN} \sum_{j=1}^m \sum_{t=1}^N
            \delta\left(x_j^{(i)}, x_j^{(t)}\right)\\
        & = \frac{mN}{mN} - \frac{1}{mN} \sum_{t=1}^N
            d\left(X^{(i)}, X^{(t)}\right)\\
        & = 1 - \frac{1}{mN} D\left(\mathcal{X}, X^{(i)}\right)
    \end{aligned}
    \end{equation}

    With this alternative definition, it is clear --- since \(m\) and \(N\) are
    fixed positive integers --- that \(\text{Dens}(X^{(i)})\) is maximised when
    \(D(\mathcal{X}, X^{(i)})\) is minimised. Then by Theorem~\ref{thm:mode},
    any data point with maximal average density is, in fact, a mode of
    \(\mathcal{X}\). This observation indicates that there is a similarity
    between this method and Huang's in that they are attempting to achieve the
    same objective if only from opposite ends.
\end{definition}

\input{tex/algorithms/cao.tex}
%\input{tex/examples/cao.tex}
