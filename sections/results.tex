\section{Experimental results}\label{sec:results}

To give comparative results on the quality of the initialisation processes 
considered in this work, four well-known, categorical, labelled datasets ---
soybean (large), mushroom, breast cancer, and zoo animal --- will be clustered
by the \(k\)-modes algorithm with each of the initialisation processes using
their respective number of classes as the number of clusters. These datasets
have been chosen to fall in line with the established literature, and for their
relative sizes and complexities. Each dataset is openly available under the
\href{http://mlr.cs.umass.edu/ml/}{UCI Machine Learning
Repository}~\cite{Dua2019}, and their characteristics are summarised in
Table~\ref{tab:dataset_summary}.

\begin{table}[htbp]
    \resizebox{\textwidth}{!}{%
        \input{tex/dataset_summary.tex}
    }\caption{A summary of the benchmark datasets.}\label{tab:dataset_summary}
\end{table}

Clustering algorithms are often evaluated based on their performance as a
classifier~\cite{%
    Arthur2007,Cao2009,Cao2012,Huang1998,Ng2007,Olaode2014,Schaeffer2007%
}. This is a fundamentally flawed approach --- especially given that
classification belongs to an entirely different branch of learning. Moreover,
doing so requires a number of assumptions about the topology of the data within
the metric space that is being considered~\cite{Memoli2011}. One such assumption
is that the classes recorded in the data are indeed separable objects like
clusters.

This analysis does not consider evaluative metrics related to classification
such as accuracy, recall or precision. Instead, only internal measures are
considered such as the cost function defined in~\eqref{eq:cost}. This metric is
label-invariant and its values are comparable across the different
initialisation methods.  Furthermore, the effect of each initialisation method
on the initial and final clusterings can be captured with the cost function. An
additional, and often useful, metric is the silhouette coefficient. This
measures the ratio between the intra-cluster cohesion and inter-cluster
separation of a particular clustering. Therefore, it could be used in a similar
way to reveal the effect of each initialisation method at the beginning and end
of a run of \(k\)-modes.  Unfortunately, this metric loses its intuition under
the distance measure employed here. The remaining performance measures used are
the number of iterations for the \(k\)-modes algorithm to terminate and the time
taken in seconds.

Tables~\ref{tab:breast_cancer_summary}---\ref{tab:soybean_summary} summarise the
results of each initialisation method on the benchmark datasets. Each column
shows the mean value of each metric and its standard deviation in parentheses
over \input{./tex/repetitions.tex} repetitions of the \(k\)-modes algorithm. The
data for these results was generated using the Python library
\href{https://github.com/nicodv/kmodes}{\texttt{kmodes}} in which the proposed
method has been implemented. All of the data included in this paper, and the
code to generate it are archived online~\textcolor{red}{code and data DOI}.

\begin{table}
    \centering
    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/breast_cancer_summary.tex}
    }
    \captionof{table}{Summative metric results for the breast cancer dataset
    with \(k=10\).}\label{tab:breast_cancer_summary}\vspace{20pt}

    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/mushroom_summary.tex}
    }
    \captionof{table}{Summative metric results for the mushroom dataset with
    \(k=4\).}\label{tab:mushroom_summary}\vspace{20pt}

    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/soybean_summary.tex}
    }
    \captionof{table}{Summative metric results for the soybean dataset with
    \(k=6\).}\label{tab:soybean_summary}\vspace{20pt}
\end{table}
