\section{Experimental results}\label{sec:results}

To give comparative results on the quality of the initialisation processes
considered in this work, four well-known, categorical, labelled datasets ---
breast cancer, mushroom, nursery, and soybean (large) --- will be clustered by
the \(k\)-modes algorithm with each of the initialisation processes. These
datasets have been chosen to fall in line with the established literature, and
for their relative sizes and complexities. Each dataset is openly available
under the UCI Machine Learning Repository~\cite{Dua2019}, and their
characteristics are summarised in Table~\ref{tab:dataset_summary}.

\begin{table}[htbp]
    \resizebox{\textwidth}{!}{%
        \input{tex/dataset_summary.tex}
    }\caption{A summary of the benchmark datasets.}\label{tab:dataset_summary}
\end{table}

Clustering algorithms are often evaluated based on their performance as a
classifier~\cite{%
    Arthur2007,Cao2009,Cao2012,Huang1998,Ng2007,Olaode2014,Schaeffer2007%
}. This is a fundamentally flawed approach --- especially given that
classification belongs to an entirely different branch of learning. Moreover,
doing so requires a number of assumptions about the topology of the data within
the metric space that is being considered~\cite{Memoli2011}. One such assumption
is that the classes recorded in the data are indeed separable objects like
clusters.

This analysis does not consider evaluative metrics related to classification
such as accuracy, recall or precision. Instead, only internal measures are
considered such as the cost function defined in~\eqref{eq:cost}. This metric is
label-invariant and its values are comparable across the different
initialisation methods. Furthermore, the effect of each initialisation method
on the initial and final clusterings can be captured with the cost function. An
additional, and often useful, metric is the silhouette coefficient. This
measures the ratio between the intra-cluster cohesion and inter-cluster
separation of a particular clustering. Therefore, it could be used in a similar
way to reveal the effect of each initialisation method at the beginning and end
of a run of \(k\)-modes. Unfortunately, this metric loses its intuition under
the distance measure employed here and is omitted. The remaining performance
measures used are the number of iterations for the \(k\)-modes algorithm to
terminate and the time taken to terminate in seconds.

The final piece of information required in this analysis is a choice for \(k\)
for each dataset. An immediate choice is the number of classes that are present
in a dataset but, as stated above, this is not necessarily a fair or wise choice
since the classes may not be representative of true clusters. A popular strategy
for choosing an optimal number of clusters is known as the `elbow' method. The
aim of this method is to identify a kink (elbow) in a plot of number of clusters
against cost for a dataset. This kink suggests that an increase in \(k\) from
there would not sufficiently improve the performance of the model. On its own,
this method is vague and somewhat unreliable which raises a number of questions:
\begin{itemize}
    \item What constitutes a kink?
    \item How does one discern between multiple kinks?
    \item Is the decision subjective with respect to the observer?
\end{itemize}

Alas, an alternative `elbow' may be identified objectively by using
the knee point detection algorithm~\cite{Satopaa2011} where the maximal
value of \(k\) is taken to be \(\lfloor\sqrt{N}\rfloor\). This algorithm
identifies the value of \(k\) with the maximum curvature in the plot described
above by computation rather than inspection --- eliminating the concerns raised.
An example plot for the nursery dataset is given in
Figure~\ref{fig:nursery_costs} where the clustering is performed using Cao's
method.

\begin{figure}
    \centering
    \includegraphics[width=.6\linewidth]{./img/elbow/nursery_costs.pdf}
    \caption{An elbow plot for the nursery dataset using Cao's initialisation
             method.}\label{fig:nursery_costs}
\end{figure}

\subsection{Elbow method}

Tables~\ref{tab:breast_cancer_summary}---\ref{tab:soybean_summary} summarise the
results of each initialisation method on the benchmark datasets. Each column
shows the mean value of each metric and its standard deviation in parentheses
over \input{./tex/repetitions.tex}repetitions of the \(k\)-modes algorithm. The
data for these results was generated using the Python library
\href{https://github.com/nicodv/kmodes}{\texttt{kmodes}} in which the proposed
method has been implemented. All of the data included in this paper, and the
code to generate it are archived online~\textcolor{red}{code and data DOI}.

\begin{table}
    \centering
    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/breast_cancer_summary.tex}
    }
    \captionof{table}{Summative metric results for the breast cancer dataset
    with \(k=8\).}\label{tab:breast_cancer_summary}\vspace{20pt}

    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/mushroom_summary.tex}
    }
    \captionof{table}{Summative metric results for the mushroom dataset with
    \(k=17\).}\label{tab:mushroom_summary}\vspace{20pt}

    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/nursery_summary.tex}
    }
    \captionof{table}{Summative metric results for the nursery dataset with
    \(k=23\).}\label{tab:nursery_summary}\vspace{20pt}

    \resizebox{\textwidth}{!}{%
        \input{./tex/elbow/soybean_summary.tex}
    }
    \captionof{table}{Summative metric results for the soybean dataset with
    \(k=8\).}\label{tab:soybean_summary}
\end{table}

\begin{figure}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/breast_cancer_cost_scatterplot.pdf}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/breast_cancer_initial_cost_cdfplot.pdf}

        \includegraphics[width=\linewidth]%
            {./img/elbow/breast_cancer_final_cost_cdfplot.pdf}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/mushroom_cost_scatterplot.pdf}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/mushroom_initial_cost_cdfplot.pdf}

        \includegraphics[width=\linewidth]%
            {./img/elbow/mushroom_final_cost_cdfplot.pdf}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/soybean_cost_scatterplot.pdf}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/soybean_initial_cost_cdfplot.pdf}

        \includegraphics[width=\linewidth]%
            {./img/elbow/soybean_final_cost_cdfplot.pdf}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/nursery_cost_scatterplot.pdf}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \includegraphics[width=\linewidth]%
            {./img/elbow/nursery_initial_cost_cdfplot.pdf}

        \includegraphics[width=\linewidth]%
            {./img/elbow/nursery_final_cost_cdfplot.pdf}
    \end{minipage}
\end{figure}
