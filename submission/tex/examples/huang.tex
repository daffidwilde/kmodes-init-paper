\begin{example}\label{ex:huang}
    Consider our vehicle dataset. We will now find a set of initial modes for
    the \(k\)-modes algorithm using Huang's method. For the sake of this
    example, we will let \(k = 3\).
    
    \begin{table}[H]
    \centering
    \singlespacing{%
    \resizebox{.8\textwidth}{!}{%
        \input{tex/relative_freq.tex}
    }}
    \caption{Relative frequency table for attribute values.}\label{tab:rel-freq}
    \end{table}

    We begin by calculating the relative frequencies of our attributes' values,
    which are stored in Table~\ref{tab:rel-freq}. Now to find our set of
    (potentially) virtual modes, \(\tilde{\mu}\). For each attribute, we will
    take a sample of size one from the its set of values according to the
    probability distribution represented in the corresponding column of
    Table~\ref{tab:rel-freq}. Let us begin with the first attribute of our first
    mode in \(\tilde{\mu}\). Then we have to sample from the following
    probability distribution:
    
    \begin{table}[H]
    \centering
    \singlespacing{%
    \begin{tabular}{cccccc}
        \(A_{1}\) &\vline& L & M & H & V \\
        \midrule\(\mathbb{P}(A_{1} = a_s^{(1)})\) &\vline& \(\frac{2}{10}\) &
        \(\frac{2}{10}\) & \(\frac{4}{10}\) & \(\frac{2}{10}\) 
    \end{tabular}
    }
    \end{table}
    
    We sample one value from this distribution and set that value to be the
    first component of our first mode. This process is repeated for all values
    \(l = 1, 2, 3\) and \(j = 1, \ldots, 6\) giving us \(3\) \(m\)-dimensional
    vectors that fairly represent the most frequent attribute values. This set
    of vectors is \(\tilde{\mu}\).

    There are many ways of obtaining \(\tilde{\mu}\) from our relative frequency
    table but we have opted to do so using a short Python script (see Appendix).
    In this case, we have the following set of vectors:

    \input{tex/huang_virtual_modes.tex}

    Finally, we take each element of \(\tilde{\mu}\) in turn and find its most
    similar point in the dataset. This collection of points in the dataset then
    forms our set of initial modes \(\bar{\mu}\) to be passed on to the 
    \(k\)-modes algorithm. We stipulate that no point which is identical to
    another that has been already selected may be used as an initial mode. This
    is done so as to avoid empty clusters further down the line.

    \begin{table}[H]
    \centering
    \singlespacing{%
    \resizebox{.8\textwidth}{!}{%
        \input{tex/huang_dissim.tex}
    }}
    \caption{The dataset ranked by dissimilarity to the first element of
    \(\tilde{\mu}\).}\label{tab:huang-mode-dissim}
    \end{table}

    Taking the first element of \(\tilde{\mu}\), we calculate the dissimilarity
    between this vector and all of our datapoints.
    Table~\ref{tab:huang-mode-dissim} shows the elements of our dataset ranked
    in ascending order of their dissimilarity to this vector. It follows that we
    should set our first initial mode to be the sixth entry of our dataset. We
    continue this process for the other elements of \(\tilde{\mu}\),
    disregarding any points that have already been selected.
    
    Using our Python implementation for Huang's initialisation method we have 
    that the set of initial modes for this instance of the \(k\)-modes algorithm
    correspond to the sixth, fifth and fourth rows of the dataset. That is, we
    have:

    \input{tex/huang_initial_modes.tex}
\end{example}


